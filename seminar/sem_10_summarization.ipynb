{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"sem_11_summarization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ws1FXPusI0Bh"},"source":["# Summarization"]},{"cell_type":"markdown","metadata":{"id":"oTgTMQb8M0aN"},"source":["## Deep Reinforced Model for Abstractive Summarization"]},{"cell_type":"markdown","metadata":{"id":"qVan4a0tNCJY"},"source":["[A Deep Reinforced Model for Abstractive Summarization (Romain Paulus, Caiming Xiong, Richard Socher, 2017)](https://arxiv.org/abs/1705.04304) - модель суммаризации на основе encoder-decoder с использованием reinforcement learning для обучения."]},{"cell_type":"markdown","metadata":{"id":"yUtEp0ZlS6xK"},"source":["### Источники\n","1. Блог с описанием статьи: https://blog.einstein.ai/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization/\n","2. Имплементация: https://github.com/rohithreddy024/Text-Summarizer-Pytorch"]},{"cell_type":"code","metadata":{"id":"--9IWPY_gK-n","executionInfo":{"status":"ok","timestamp":1604766236667,"user_tz":-180,"elapsed":2341,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"686b1ae4-54ea-4caa-8299-bdfc9e763e34","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone https://github.com/rohithreddy024/Text-Summarizer-Pytorch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Text-Summarizer-Pytorch'...\n","remote: Enumerating objects: 98, done.\u001b[K\n","remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98\u001b[K\n","Unpacking objects: 100% (98/98), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VSI7sgTDiks1"},"source":["### Данные \n","Модель обучается на данных Gigaword dataset: https://github.com/harvardnlp/sent-summary"]},{"cell_type":"code","metadata":{"id":"TUWvD95NVaTZ","executionInfo":{"status":"ok","timestamp":1604766242902,"user_tz":-180,"elapsed":893,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["# find the share link of the file/folder on Google Drive\n","file_share_link = \"https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\"\n","\n","# extract the ID of the file\n","file_id = file_share_link[file_share_link.find(\"=\") + 1:]\n","\n","# append the id to this REST command\n","file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"oe4k_BLYYJot","executionInfo":{"status":"ok","timestamp":1604766254341,"user_tz":-180,"elapsed":2348,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"61e5e652-1ecb-4dcf-c291-0f38f1119f32","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["file_id"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0B6N7tANPyVeBNmlSX19Ld2xDU1E'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"OYOUzkNGXl4t","executionInfo":{"status":"ok","timestamp":1604766264390,"user_tz":-180,"elapsed":7491,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"2f892474-1434-4572-81c5-41c0b34d61c0","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\" -O summary.tar.gz && rm -rf /tmp/cookies.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-11-07 16:24:21--  https://docs.google.com/uc?export=download&confirm=kkak&id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n","Resolving docs.google.com (docs.google.com)... 172.217.5.238, 2607:f8b0:4004:809::200e\n","Connecting to docs.google.com (docs.google.com)|172.217.5.238|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download [following]\n","--2020-11-07 16:24:21--  https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download\n","Resolving doc-10-0s-docs.googleusercontent.com (doc-10-0s-docs.googleusercontent.com)... 172.217.15.97, 2607:f8b0:4004:811::2001\n","Connecting to doc-10-0s-docs.googleusercontent.com (doc-10-0s-docs.googleusercontent.com)|172.217.15.97|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://docs.google.com/nonceSigner?nonce=v8lncjposv00c&continue=https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e%3Ddownload&hash=e12baosjnv7fpiffprk1c9q7h8gcat8m [following]\n","--2020-11-07 16:24:21--  https://docs.google.com/nonceSigner?nonce=v8lncjposv00c&continue=https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e%3Ddownload&hash=e12baosjnv7fpiffprk1c9q7h8gcat8m\n","Connecting to docs.google.com (docs.google.com)|172.217.5.238|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download&nonce=v8lncjposv00c&user=09442447318250874164Z&hash=bhanermikes7prceqmud7hig5e4e1k37 [following]\n","--2020-11-07 16:24:21--  https://doc-10-0s-docs.googleusercontent.com/docs/securesc/vap1rridkhuhd0re3ocuc8hn139v6rp0/sgd4hnuddek2mr6a1sktuavgmfnrp02g/1604766225000/03129501499031348422/09442447318250874164Z/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download&nonce=v8lncjposv00c&user=09442447318250874164Z&hash=bhanermikes7prceqmud7hig5e4e1k37\n","Connecting to doc-10-0s-docs.googleusercontent.com (doc-10-0s-docs.googleusercontent.com)|172.217.15.97|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/gzip]\n","Saving to: ‘summary.tar.gz’\n","\n","summary.tar.gz          [                <=> ] 277.39M  67.1MB/s    in 4.1s    \n","\n","2020-11-07 16:24:26 (67.1 MB/s) - ‘summary.tar.gz’ saved [290866023]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LdA8FJ-YT3Jl","executionInfo":{"status":"ok","timestamp":1604766283199,"user_tz":-180,"elapsed":3039,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"f935b88e-98ad-4ae7-f9b0-94a36ea921cc","colab":{"base_uri":"https://localhost:8080/"}},"source":["!tar -tvf summary.tar.gz"],"execution_count":5,"outputs":[{"output_type":"stream","text":["drwxrwxr-x srush/srush       0 2016-04-12 15:46 sumdata/\n","drwxrwxr-x srush/srush       0 2016-04-12 15:27 sumdata/DUC2004/\n","-rw-r--r-- srush/srush  103953 2016-04-12 15:27 sumdata/DUC2004/input.txt\n","-rw-r--r-- srush/srush   36083 2016-04-12 15:27 sumdata/DUC2004/task1_ref3.txt\n","-rw-r--r-- srush/srush   34426 2016-04-12 15:27 sumdata/DUC2004/task1_ref0.txt\n","-rw-r--r-- srush/srush   35530 2016-04-12 15:27 sumdata/DUC2004/task1_ref1.txt\n","-rw-r--r-- srush/srush   35967 2016-04-12 15:27 sumdata/DUC2004/task1_ref2.txt\n","drwxrwxr-x srush/srush       0 2016-04-12 15:30 sumdata/Giga/\n","-rw-rw-r-- srush/srush  335302 2016-04-12 15:25 sumdata/Giga/input.txt\n","-rw-rw-r-- srush/srush  103766 2016-04-12 15:28 sumdata/Giga/task1_ref0.txt\n","drwxrwxr-x srush/srush       0 2016-04-12 15:46 sumdata/DUC2003/\n","-rw-r--r-- srush/srush  128364 2016-04-12 15:46 sumdata/DUC2003/input.txt\n","-rw-r--r-- srush/srush   48910 2016-04-12 15:46 sumdata/DUC2003/task1_ref3.txt\n","-rw-r--r-- srush/srush   43027 2016-04-12 15:46 sumdata/DUC2003/task1_ref0.txt\n","-rw-r--r-- srush/srush   45454 2016-04-12 15:46 sumdata/DUC2003/task1_ref1.txt\n","-rw-r--r-- srush/srush   45721 2016-04-12 15:46 sumdata/DUC2003/task1_ref2.txt\n","drwxrwxr-x srush/srush       0 2016-04-12 15:28 sumdata/train/\n","-rw-r--r-- srush/srush 64357057 2016-04-12 13:40 sumdata/train/train.title.txt.gz\n","-rw-r--r-- srush/srush 34664441 2016-04-12 13:41 sumdata/train/valid.article.filter.txt\n","-rw-r--r-- srush/srush 10046437 2016-04-12 13:41 sumdata/train/valid.title.filter.txt\n","-rw-r--r-- srush/srush 211930097 2016-04-12 13:36 sumdata/train/train.article.txt.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"atSkLKu7YUbo","executionInfo":{"status":"ok","timestamp":1604766298219,"user_tz":-180,"elapsed":11433,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["!tar -xf summary.tar.gz sumdata/train/\n","!gunzip sumdata/train/train.article.txt.gz\n","!gunzip sumdata/train/train.title.txt.gz\n","! mv sumdata/train/* Text-Summarizer-Pytorch/data/unfinished\n","!rm -rf sumdata/"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPtC-lUgg60A","executionInfo":{"status":"ok","timestamp":1604766303280,"user_tz":-180,"elapsed":980,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"23375430-6f36-4314-e520-b3975a255c9c","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd Text-Summarizer-Pytorch"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/Text-Summarizer-Pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ULtgqMeehNiv","executionInfo":{"status":"ok","timestamp":1604766502474,"user_tz":-180,"elapsed":195733,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"e1c318f2-550d-4428-fb72-fd2027a90f3e","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Создание .bin файлов с данными для обучения модели\n","!python2 make_data_files.py"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Completed shuffling train & valid text files\n","3803957it [02:43, 23208.87it/s]\n","189651it [00:03, 60835.47it/s]\n","Completed creating bin file for train & valid\n","Completed chunking main bin files into smaller ones\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FK3MRVgZ2O5H"},"source":["Примеры обучающих данных (заголовки и абстракты статей):"]},{"cell_type":"code","metadata":{"id":"9VmxqtFmzKIO","executionInfo":{"status":"ok","timestamp":1604767780274,"user_tz":-180,"elapsed":965,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["N = 20\n","with open(\"data/unfinished/train.article.txt\") as f:\n","    head = [next(f) for x in range(N)]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0El6aWazoUA","executionInfo":{"status":"ok","timestamp":1604767785420,"user_tz":-180,"elapsed":1102,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"c88f3244-10eb-4791-efcc-8ad48d020178","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(head[1])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vH46yo-8z1G4","executionInfo":{"status":"ok","timestamp":1604767792163,"user_tz":-180,"elapsed":1069,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"d9a7ed7d-2031-4c24-cbf2-7371592a10e2","colab":{"base_uri":"https://localhost:8080/"}},"source":["N = 20\n","with open(\"data/unfinished/train.title.txt\") as f:\n","    head = [next(f) for x in range(N)]\n","print(head[1])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["at least two dead in southern philippines blast\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"woOtaany2yVS"},"source":["### Модель\n","Идея состоит в обучении encoder-decoder архитектуры для генерации summary входного текста. \n","\n","В декодере дважды используется механизм attention:\n","1. attention на состояния энкодера (intra-temporal attention) определяет вес слов входной последовательности для текущей позиции в выходной последовательности summary\n","2. attention на предыдущие состояния декодера (intra-decoder attention) для того, чтобы не допускать повторения слов в выходе декодера.\n","\n","В процессе обучения модели используется teacher-forcing, чтобы учитывать ошибку на уровне каждого генерируемого слова (Negative Log Likelihood Loss), и reinforcement learning для оценки качества сгенерированного текста целиком в сравнении с target summary. \n","\n","Для reinforcement learning в качестве метрики используется ROUGE score. ROUGE считает совпадение н-грамм слов в таргете и сгенерированной последовательности (ROUGE-1 для униграмм, ROUGE-2 для биграмм слов, ...). \n"]},{"cell_type":"markdown","metadata":{"id":"iD-rqVkqfIxF"},"source":["![summ_attentions](summ-attentions.svg)"]},{"cell_type":"code","metadata":{"id":"hKDtgnWLmxlg","executionInfo":{"status":"ok","timestamp":1604767848762,"user_tz":-180,"elapsed":3879,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"82149b0d-0d2a-40b3-8207-b2cf8faeb4a8","colab":{"base_uri":"https://localhost:8080/"}},"source":["# rouge для подчета метрики Rouge\n","!pip install rouge"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting rouge\n","  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0c7KO5v3C730"},"source":["Обучение модели (train.py)"]},{"cell_type":"code","metadata":{"id":"XCMPOhhRCZ7Z","executionInfo":{"status":"ok","timestamp":1604768209254,"user_tz":-180,"elapsed":1791,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    #Set cuda device\n","\n","import time\n","\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from model import Model\n","\n","from data_util import config, data\n","from data_util.batcher import Batcher\n","from data_util.data import Vocab\n","from train_util import *\n","from torch.distributions import Categorical\n","from rouge import Rouge\n","from numpy import random\n","import argparse\n","\n","random.seed(123)\n","T.manual_seed(123)\n","if T.cuda.is_available():\n","    T.cuda.manual_seed_all(123)\n","\n","class Train(object):\n","    def __init__(self, opt):\n","        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n","        self.batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n","                               batch_size=config.batch_size, single_pass=False)\n","        self.opt = opt\n","        self.start_id = self.vocab.word2id(data.START_DECODING)\n","        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n","        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n","        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n","        time.sleep(5)\n","\n","    def save_model(self, iter):\n","        save_path = config.save_model_path + \"/%07d.tar\" % iter\n","        T.save({\n","            \"iter\": iter + 1,\n","            \"model_dict\": self.model.state_dict(),\n","            \"trainer_dict\": self.trainer.state_dict()\n","        }, save_path)\n","\n","    def setup_train(self):\n","        self.model = Model()\n","        self.model = get_cuda(self.model)\n","        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n","        start_iter = 0\n","        if self.opt.load_model is not None:\n","            load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n","            checkpoint = T.load(load_model_path)\n","            start_iter = checkpoint[\"iter\"]\n","            self.model.load_state_dict(checkpoint[\"model_dict\"])\n","            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n","            print(\"Loaded model at \" + load_model_path)\n","        if self.opt.new_lr is not None:\n","            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n","        return start_iter\n","\n","    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n","        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n","                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n","        Args:\n","        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n","        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n","        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n","        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n","        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n","        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n","        :param batch: batch object\n","        '''\n","        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n","        step_losses = []\n","        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n","        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n","        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n","        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n","        for t in range(min(max_dec_len, config.max_dec_steps)):\n","            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n","            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n","            x_t = self.model.embeds(x_t)\n","            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n","            target = target_batch[:, t]\n","            log_probs = T.log(final_dist + config.eps)\n","            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n","            step_losses.append(step_loss)\n","            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n","            is_oov = (x_t >= config.vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n","            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n","\n","        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n","        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n","        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n","        return mle_loss\n","\n","    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n","        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n","        Args\n","        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n","        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n","        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n","        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n","        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n","        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n","        :param article_oovs: Batch containing list of OOVs in each example\n","        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n","        Returns:\n","        :decoded_strs: List of decoded sentences\n","        :log_probs: Log probabilities of sampled words\n","        '''\n","        s_t = enc_hidden                                                                            #Decoder hidden states\n","        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n","        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n","        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n","        inds = []                                                                                   #Stores sampled indices for each time step\n","        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n","        log_probs = []                                                                              #Stores log probabilites of generated samples\n","        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n","\n","        for t in range(config.max_dec_steps):\n","            x_t = self.model.embeds(x_t)\n","            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n","            if greedy is False:\n","                multi_dist = Categorical(probs)\n","                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n","                log_prob = multi_dist.log_prob(x_t)\n","                log_probs.append(log_prob)\n","            else:\n","                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n","            x_t = x_t.detach()\n","            inds.append(x_t)\n","            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n","            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n","            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n","            decoder_padding_mask.append(mask_t)\n","            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n","            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n","\n","        inds = T.stack(inds, dim=1)\n","        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n","        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n","            log_probs = T.stack(log_probs, dim=1)\n","            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n","            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n","            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n","        decoded_strs = []\n","        for i in range(len(enc_out)):\n","            id_list = inds[i].cpu().numpy()\n","            oovs = article_oovs[i]\n","            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n","            try:\n","                end_idx = S.index(data.STOP_DECODING)\n","                S = S[:end_idx]\n","            except ValueError:\n","                S = S\n","            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n","                S = [\"xxx\"]\n","            S = \" \".join(S)\n","            decoded_strs.append(S)\n","\n","        return decoded_strs, log_probs\n","\n","    def reward_function(self, decoded_sents, original_sents):\n","        rouge = Rouge()\n","        try:\n","            scores = rouge.get_scores(decoded_sents, original_sents)\n","        except Exception:\n","            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n","            scores = []\n","            for i in range(len(decoded_sents)):\n","                try:\n","                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n","                except Exception:\n","                    print(\"Error occured at:\")\n","                    print(\"decoded_sents:\", decoded_sents[i])\n","                    print(\"original_sents:\", original_sents[i])\n","                    score = [{\"rouge-l\":{\"f\":0.0}}]\n","                scores.append(score[0])\n","        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n","        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n","        return rouge_l_f1\n","\n","    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n","    #     with open(\"temp.txt\", \"w\") as f:\n","    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n","    #         for i in range(len(original)):\n","    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n","    #             f.write(\"max: \"+max[i]+\"\\n\")\n","    #             f.write(\"org: \"+original[i]+\"\\n\")\n","    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n","\n","\n","    def train_one_batch(self, batch, iter):\n","        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n","\n","        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n","        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n","\n","        # -------------------------------Summarization-----------------------\n","        if self.opt.train_mle == \"yes\":                                                             #perform MLE training\n","            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n","        else:\n","            mle_loss = get_cuda(T.FloatTensor([0]))\n","        # --------------RL training-----------------------------------------------------\n","        if self.opt.train_rl == \"yes\":                                                              #perform reinforcement learning training\n","            # multinomial sampling\n","            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n","            with T.autograd.no_grad():\n","                # greedy sampling\n","                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n","\n","            sample_reward = self.reward_function(sample_sents, batch.original_abstracts)\n","            baseline_reward = self.reward_function(greedy_sents, batch.original_abstracts)\n","            # if iter%200 == 0:\n","            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n","            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n","            rl_loss = T.mean(rl_loss)\n","\n","            batch_reward = T.mean(sample_reward).item()\n","        else:\n","            rl_loss = get_cuda(T.FloatTensor([0]))\n","            batch_reward = 0\n","\n","    # ------------------------------------------------------------------------------------\n","        self.trainer.zero_grad()\n","        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n","        self.trainer.step()\n","\n","        return mle_loss.item(), batch_reward\n","\n","    def trainIters(self):\n","        iter = self.setup_train()\n","        count = mle_total = r_total = 0\n","        while iter <= config.max_iterations:\n","            batch = self.batcher.next_batch()\n","            try:\n","                mle_loss, r = self.train_one_batch(batch, iter)\n","            except KeyboardInterrupt:\n","                print(\"-------------------Keyboard Interrupt------------------\")\n","                exit(0)\n","\n","            mle_total += mle_loss\n","            r_total += r\n","            count += 1\n","            iter += 1\n","\n","            if iter % 1000 == 0:\n","                mle_avg = mle_total / count\n","                r_avg = r_total / count\n","                print(\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n","                count = mle_total = r_total = 0\n","\n","            if iter % 5000 == 0:\n","                self.save_model(iter)\n","\n","\n","# parser = argparse.ArgumentParser()\n","# parser.add_argument('--train_mle', type=str, default=\"yes\")\n","# parser.add_argument('--train_rl', type=str, default=\"no\")\n","# parser.add_argument('--mle_weight', type=float, default=1.0)\n","# parser.add_argument('--load_model', type=str, default=None)\n","# parser.add_argument('--new_lr', type=float, default=None)\n","# opt = parser.parse_args()\n","# opt.rl_weight = 1 - opt.mle_weight\n","# print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n","# print(\"intra_encoder:\", config.intra_encoder, \"intra_decoder:\", config.intra_decoder)\n","\n","# train_processor = Train(opt)\n","# train_processor.trainIters()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D4SsF_7BDQwF"},"source":["Сначала encoder-decoder модель обучается без reinforcement learning. "]},{"cell_type":"code","metadata":{"id":"1kUSZGFojE9K","executionInfo":{"status":"ok","timestamp":1604768430394,"user_tz":-180,"elapsed":213299,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"30672544-a69c-4290-c70a-6d8e7393ba35","colab":{"base_uri":"https://localhost:8080/"}},"source":["!python train.py --train_mle=yes --train_rl=no --mle_weight=1.0"],"execution_count":15,"outputs":[{"output_type":"stream","text":["2020-11-07 16:57:01.568167: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","Training mle: yes, Training rl: no, mle weight: 1.00, rl weight: 0.00\n","intra_encoder: True intra_decoder: True\n","Exception in thread Thread-3:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/Text-Summarizer-Pytorch/data_util/batcher.py\", line 247, in watch_threads\n","    tf.logging.info(\n","AttributeError: module 'tensorflow' has no attribute 'logging'\n","\n","-------------------Keyboard Interrupt------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r049heCmGAIT"},"source":["Выбирается лучшая модель (из обученных с разным числом итераций) по значению ROUGE на валидационной выборке."]},{"cell_type":"code","metadata":{"id":"TStUf1SLmh4l"},"source":["!python eval.py --task=validate --start_from=0005000.tar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTq3MdGtGE01"},"source":["Лучшая модель дообучается с использованием RL."]},{"cell_type":"code","metadata":{"id":"gu-XLAm8Fe63"},"source":["# MLE + RL training\n","!python train.py --train_mle=yes --train_rl=yes --mle_weight=0.25 --load_model=0100000.tar --new_lr=0.0001 \n","\n","# RL training\n","python train.py --train_mle=no --train_rl=yes --mle_weight=0.0 --load_model=0100000.tar --new_lr=0.0001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vB-YSeGhGnrK"},"source":["Модель, обученная только на RL, достигает более высоких показателей ROUGE, но генерирует менее хорошие тексты с точки зрения связности и естественности, поэтому авторы статьи рекомендуют комбинированную стратегию обучения. "]},{"cell_type":"markdown","metadata":{"id":"yVIZJv5uH7i_"},"source":["* Результаты, приведенные авторами репозитория:"]},{"cell_type":"markdown","metadata":{"id":"nO0bhu9wHgGn"},"source":["Rouge scores obtained by using best MLE trained model on test set:\n","\n","{\n","'rouge-1': {'f': 0.4412018559893622, 'p': 0.4814799494024485, 'r': 0.4232331027817015}, \n","\n","'rouge-2': {'f': 0.23238981595683728, 'p': 0.2531296070596062, 'r': 0.22407861554997008},\n","\n","'rouge-l': {'f': 0.40477682528278364, 'p': 0.4584684491434479, 'r': 0.40351107200202596}\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"PeUQzzvNHzPc"},"source":["Rouge scores obtained by using best MLE + RL trained model on test set:\n","\n","{\n","'rouge-1': {'f': 0.4499047033247696, 'p': 0.4853756369556345, 'r': 0.43544461386607497},\n","\n","'rouge-2': {'f': 0.24037014314625643, 'p': 0.25903387205387235, 'r': 0.23362662645146298},\n","\n","'rouge-l': {'f': 0.41320241732946406, 'p': 0.4616655167980162, 'r': 0.4144419466382236}\n","}"]},{"cell_type":"markdown","metadata":{"id":"NiFAGr6aIbIN"},"source":["* Примеры (article - исходный текст, ref - target summary, dec - сгенерированный моделью текст):"]},{"cell_type":"markdown","metadata":{"id":"KsCPWCkTIa8M"},"source":["article: russia 's lower house of parliament was scheduled friday to debate an appeal to the prime minister that challenged the right of u.s.-funded radio liberty to operate in russia following its introduction of broadcasts targeting chechnya .\n","\n","ref: russia 's lower house of parliament mulls challenge to radio liberty\n","\n","dec: russian parliament to debate on banning radio liberty\n"]},{"cell_type":"markdown","metadata":{"id":"JaW__-pIJBYT"},"source":["article: continued dialogue with the democratic people 's republic of korea is important although australia 's plan to open its embassy in pyongyang has been shelved because of the crisis over the dprk 's nuclear weapons program , australian foreign minister alexander downer said on friday .\n","\n","ref: dialogue with dprk important says australian foreign minister\n","\n","dec: australian fm says dialogue with dprk important"]},{"cell_type":"markdown","metadata":{"id":"PkMDr8zAJW3o"},"source":["article: water levels in the zambezi river are rising due to heavy rains in its catchment area , prompting zimbabwe 's civil protection unit -lrb- cpu -rrb- to issue a flood alert for people living in the zambezi valley , the herald reported on friday .\n","\n","ref: floods loom in zambezi valley\n","\n","dec: water levels rising in zambezi river"]},{"cell_type":"code","metadata":{"id":"i2KBeWCXJvT0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4W38g90JvAS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VcvVX9YJon0"},"source":["## BERT Extractive Summarization"]},{"cell_type":"markdown","metadata":{"id":"uCOikZ1yJyDo"},"source":["### Источник:\n","https://deeplearninganalytics.org/text-summarization/\n","\n","https://github.com/nlpyang/BertSum\n"]},{"cell_type":"markdown","metadata":{"id":"aPWErqGiSpGZ"},"source":["Идея: использовать BERT эмбеддинги предложений исходного текста в задаче бинарной классификации для отбора самых значимых предложений, которые войдут в summary.\n","\n","Для получения эмбеддингов нескольких предложений текста перед каждым предложением текста вставляется свой токен начала предложения **[CLS]**, после каждого предложения - символ **[SEP]**. В качестве эмбеддингов сегмента предложения (которые используются для того, чтобы различать первое и второе предложения в парах предложений при обучении  BERT) для последовательности предложений чередуются единичные и нулевые вектора.\n","\n","_[sent1, sent2, sent3, sent4, sent5] -> [EA, EB, EA, EB, EA]._\n","\n","Вектора токенов [CLS] на последнем слое BERT используются в качестве векторов предложений текста. Вектора предложений подаются на вход классификатору (в статье 3 варианта классификации): \n","1. linear layer + sigmoid\n","2. Transformer + sigmoid\n","3. LSTM + sigmoid"]},{"cell_type":"markdown","metadata":{"id":"y_z1kBmMfI0e"},"source":["![bertsum](bertsum.png)"]},{"cell_type":"code","metadata":{"id":"qgVjUukfgYAr","executionInfo":{"status":"ok","timestamp":1604768747036,"user_tz":-180,"elapsed":142324,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"60f0a5fd-7911-4c83-c6de-d642a021032d","colab":{"base_uri":"https://localhost:8080/","height":458}},"source":["!pip install --force-reinstall torch==1.1.0"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Collecting torch==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n","\u001b[K     |████████████████████████████████| 676.9MB 26kB/s \n","\u001b[?25hCollecting numpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 223kB/s \n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, torch\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","Successfully installed numpy-1.19.4 torch-1.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","torch"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"I3rNua2xJsxN","executionInfo":{"status":"ok","timestamp":1604768791440,"user_tz":-180,"elapsed":7915,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"743eae86-acfa-45f3-fe72-c2dd01f31f9f","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pytorch-pretrained-bert"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 26.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 24.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 18.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 11.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 11.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/61/0b49baae16f482156550ce0b78a7ad265c27188a9d4fe6a1bd741fb43b9d/boto3-1.16.13.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 9.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.19.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Collecting botocore<1.20.0,>=1.19.13\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/40/b5e681d80dc46bafd0dc2e55266190cc432dfd5b72b9e7e1c5743aa6c362/botocore-1.19.13-py2.py3-none-any.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 22.7MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.13->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.13->boto3->pytorch-pretrained-bert) (1.15.0)\n","Building wheels for collected packages: boto3\n","  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for boto3: filename=boto3-1.16.13-py2.py3-none-any.whl size=128453 sha256=bdf121fdc77e33dfcfdb80dffb082cbf784eedf94d1ab692bda0ccff21687c06\n","  Stored in directory: /root/.cache/pip/wheels/bd/ca/cc/d99cff66806b87034af25f8fd8b0adb3e0151b17eea7891143\n","Successfully built boto3\n","\u001b[31mERROR: botocore 1.19.13 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.16.13 botocore-1.19.13 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"px7VPlV8YOxq","executionInfo":{"status":"ok","timestamp":1604768799599,"user_tz":-180,"elapsed":3686,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"4d4cebd9-e4c9-43ee-8f9b-541e1132a5b4","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install tensorboardX"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\r\u001b[K     |█                               | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 16.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 11.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 9.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qfCY-rAGYy5E","executionInfo":{"status":"ok","timestamp":1604768806813,"user_tz":-180,"elapsed":4761,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"72f5b333-ba71-42f0-c85d-245a4d39c196","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pyrouge\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Collecting pyrouge\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n","\r\u001b[K     |█████▍                          | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyrouge\n","  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=22a94db50597fededb20b1f7822fc6a4e9531ca33696289ee453bbbe6b0fa570\n","  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n","Successfully built pyrouge\n","Installing collected packages: pyrouge\n","Successfully installed pyrouge-0.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iX0FyN5nY9le","executionInfo":{"status":"ok","timestamp":1604768815437,"user_tz":-180,"elapsed":3112,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"6ed168eb-a1a6-4d49-bbd7-c89722a0b7ae","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install multiprocess"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (0.70.10)\n","Requirement already satisfied: dill>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from multiprocess) (0.3.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D5M17ucVaPkf"},"source":["### Данные"]},{"cell_type":"markdown","metadata":{"id":"bpH5yBI4ckF_"},"source":["Датасет CNN and Daily Mail \n","\n","Загрузим предобработанные данные."]},{"cell_type":"code","metadata":{"id":"77qiS30hafOT","executionInfo":{"status":"ok","timestamp":1604768821376,"user_tz":-180,"elapsed":855,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"250158e4-23d1-4f0b-d121-a568fbebe11c","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd .."],"execution_count":21,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9uXL_7cVkV-f","executionInfo":{"status":"ok","timestamp":1604768835984,"user_tz":-180,"elapsed":12064,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"4b77ca71-eb1d-4d0c-b29a-b98702dca03f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# !wget --no-check-certificate --load-cookies /tmp/cookies.txt \"http://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'http://docs.google.com/uc?export=download&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1WE9ZHAW64zKfU41KmUXsLda7zDfT9EqN\" -O bertsum_data.zip && rm -rf /tmp/cookies.txt\n","!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"http://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'http://docs.google.com/uc?export=download&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\" -O bertsum_data.zip && rm -rf /tmp/cookies.txt"],"execution_count":22,"outputs":[{"output_type":"stream","text":["URL transformed to HTTPS due to an HSTS policy\n","--2020-11-07 17:07:07--  https://docs.google.com/uc?export=download&confirm=JL3y&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\n","Resolving docs.google.com (docs.google.com)... 172.217.7.206, 2607:f8b0:4004:806::200e\n","Connecting to docs.google.com (docs.google.com)|172.217.7.206|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download [following]\n","--2020-11-07 17:07:07--  https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download\n","Resolving doc-0o-50-docs.googleusercontent.com (doc-0o-50-docs.googleusercontent.com)... 172.217.15.97, 2607:f8b0:4004:811::2001\n","Connecting to doc-0o-50-docs.googleusercontent.com (doc-0o-50-docs.googleusercontent.com)|172.217.15.97|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://docs.google.com/nonceSigner?nonce=rjpsikmud5lg4&continue=https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e%3Ddownload&hash=rlkf4f5igr4555lotqu3q0qll58u8rpg [following]\n","--2020-11-07 17:07:07--  https://docs.google.com/nonceSigner?nonce=rjpsikmud5lg4&continue=https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e%3Ddownload&hash=rlkf4f5igr4555lotqu3q0qll58u8rpg\n","Connecting to docs.google.com (docs.google.com)|172.217.7.206|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download&nonce=rjpsikmud5lg4&user=08931067858893519651Z&hash=7frvdh1kgkhgs5essd9ho9j5j3fjgcuj [following]\n","--2020-11-07 17:07:08--  https://doc-0o-50-docs.googleusercontent.com/docs/securesc/hrliplfd19njvlcnuv5un2pq1k1gbuhq/2c55arvbeeulvhc21vjdj3s5d16hhhre/1604768775000/02403291851892694101/08931067858893519651Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download&nonce=rjpsikmud5lg4&user=08931067858893519651Z&hash=7frvdh1kgkhgs5essd9ho9j5j3fjgcuj\n","Connecting to doc-0o-50-docs.googleusercontent.com (doc-0o-50-docs.googleusercontent.com)|172.217.15.97|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/zip]\n","Saving to: ‘bertsum_data.zip’\n","\n","bertsum_data.zip        [      <=>           ] 829.12M  98.0MB/s    in 10s     \n","\n","2020-11-07 17:07:18 (82.6 MB/s) - ‘bertsum_data.zip’ saved [869392410]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lK31E4MAdCEH","executionInfo":{"status":"ok","timestamp":1604768846148,"user_tz":-180,"elapsed":3091,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"a10a7e56-38e2-4b53-8107-ff0f1ffa794c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone https://github.com/nlpyang/BertSum"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Cloning into 'BertSum'...\n","remote: Enumerating objects: 301, done.\u001b[K\n","remote: Total 301 (delta 0), reused 0 (delta 0), pack-reused 301\u001b[K\n","Receiving objects: 100% (301/301), 15.03 MiB | 19.74 MiB/s, done.\n","Resolving deltas: 100% (174/174), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ERlE4EYdN9K","executionInfo":{"status":"ok","timestamp":1604768849720,"user_tz":-180,"elapsed":746,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"aa04e67b-c1f0-40e8-d9be-234a88f38ecb","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd BertSum"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/content/BertSum\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yo0WKWbWiPJ5","executionInfo":{"status":"ok","timestamp":1604768878933,"user_tz":-180,"elapsed":27368,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"b12b736e-2d9c-4eda-f3e9-de437c9465f9","colab":{"base_uri":"https://localhost:8080/"}},"source":["!unzip ../bertsum_data.zip -d ./bert_data"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Archive:  ../bertsum_data.zip\n","  inflating: ./bert_data/cnndm.test.0.bert.pt  \n","  inflating: ./bert_data/cnndm.test.1.bert.pt  \n","  inflating: ./bert_data/cnndm.test.2.bert.pt  \n","  inflating: ./bert_data/cnndm.test.3.bert.pt  \n","  inflating: ./bert_data/cnndm.test.4.bert.pt  \n","  inflating: ./bert_data/cnndm.test.5.bert.pt  \n","  inflating: ./bert_data/cnndm.train.0.bert.pt  \n","  inflating: ./bert_data/cnndm.train.100.bert.pt  \n","  inflating: ./bert_data/cnndm.train.101.bert.pt  \n","  inflating: ./bert_data/cnndm.train.102.bert.pt  \n","  inflating: ./bert_data/cnndm.train.103.bert.pt  \n","  inflating: ./bert_data/cnndm.train.104.bert.pt  \n","  inflating: ./bert_data/cnndm.train.105.bert.pt  \n","  inflating: ./bert_data/cnndm.train.106.bert.pt  \n","  inflating: ./bert_data/cnndm.train.107.bert.pt  \n","  inflating: ./bert_data/cnndm.train.108.bert.pt  \n","  inflating: ./bert_data/cnndm.train.109.bert.pt  \n","  inflating: ./bert_data/cnndm.train.10.bert.pt  \n","  inflating: ./bert_data/cnndm.train.110.bert.pt  \n","  inflating: ./bert_data/cnndm.train.111.bert.pt  \n","  inflating: ./bert_data/cnndm.train.112.bert.pt  \n","  inflating: ./bert_data/cnndm.train.113.bert.pt  \n","  inflating: ./bert_data/cnndm.train.114.bert.pt  \n","  inflating: ./bert_data/cnndm.train.115.bert.pt  \n","  inflating: ./bert_data/cnndm.train.116.bert.pt  \n","  inflating: ./bert_data/cnndm.train.117.bert.pt  \n","  inflating: ./bert_data/cnndm.train.118.bert.pt  \n","  inflating: ./bert_data/cnndm.train.119.bert.pt  \n","  inflating: ./bert_data/cnndm.train.11.bert.pt  \n","  inflating: ./bert_data/cnndm.train.120.bert.pt  \n","  inflating: ./bert_data/cnndm.train.121.bert.pt  \n","  inflating: ./bert_data/cnndm.train.122.bert.pt  \n","  inflating: ./bert_data/cnndm.train.123.bert.pt  \n","  inflating: ./bert_data/cnndm.train.124.bert.pt  \n","  inflating: ./bert_data/cnndm.train.125.bert.pt  \n","  inflating: ./bert_data/cnndm.train.126.bert.pt  \n","  inflating: ./bert_data/cnndm.train.127.bert.pt  \n","  inflating: ./bert_data/cnndm.train.128.bert.pt  \n","  inflating: ./bert_data/cnndm.train.129.bert.pt  \n","  inflating: ./bert_data/cnndm.train.12.bert.pt  \n","  inflating: ./bert_data/cnndm.train.130.bert.pt  \n","  inflating: ./bert_data/cnndm.train.131.bert.pt  \n","  inflating: ./bert_data/cnndm.train.132.bert.pt  \n","  inflating: ./bert_data/cnndm.train.133.bert.pt  \n","  inflating: ./bert_data/cnndm.train.134.bert.pt  \n","  inflating: ./bert_data/cnndm.train.135.bert.pt  \n","  inflating: ./bert_data/cnndm.train.136.bert.pt  \n","  inflating: ./bert_data/cnndm.train.137.bert.pt  \n","  inflating: ./bert_data/cnndm.train.138.bert.pt  \n","  inflating: ./bert_data/cnndm.train.139.bert.pt  \n","  inflating: ./bert_data/cnndm.train.13.bert.pt  \n","  inflating: ./bert_data/cnndm.train.140.bert.pt  \n","  inflating: ./bert_data/cnndm.train.141.bert.pt  \n","  inflating: ./bert_data/cnndm.train.142.bert.pt  \n","  inflating: ./bert_data/cnndm.train.143.bert.pt  \n","  inflating: ./bert_data/cnndm.train.14.bert.pt  \n","  inflating: ./bert_data/cnndm.train.15.bert.pt  \n","  inflating: ./bert_data/cnndm.train.16.bert.pt  \n","  inflating: ./bert_data/cnndm.train.17.bert.pt  \n","  inflating: ./bert_data/cnndm.train.18.bert.pt  \n","  inflating: ./bert_data/cnndm.train.19.bert.pt  \n","  inflating: ./bert_data/cnndm.train.1.bert.pt  \n","  inflating: ./bert_data/cnndm.train.20.bert.pt  \n","  inflating: ./bert_data/cnndm.train.21.bert.pt  \n","  inflating: ./bert_data/cnndm.train.22.bert.pt  \n","  inflating: ./bert_data/cnndm.train.23.bert.pt  \n","  inflating: ./bert_data/cnndm.train.24.bert.pt  \n","  inflating: ./bert_data/cnndm.train.25.bert.pt  \n","  inflating: ./bert_data/cnndm.train.26.bert.pt  \n","  inflating: ./bert_data/cnndm.train.27.bert.pt  \n","  inflating: ./bert_data/cnndm.train.28.bert.pt  \n","  inflating: ./bert_data/cnndm.train.29.bert.pt  \n","  inflating: ./bert_data/cnndm.train.2.bert.pt  \n","  inflating: ./bert_data/cnndm.train.30.bert.pt  \n","  inflating: ./bert_data/cnndm.train.31.bert.pt  \n","  inflating: ./bert_data/cnndm.train.32.bert.pt  \n","  inflating: ./bert_data/cnndm.train.33.bert.pt  \n","  inflating: ./bert_data/cnndm.train.34.bert.pt  \n","  inflating: ./bert_data/cnndm.train.35.bert.pt  \n","  inflating: ./bert_data/cnndm.train.36.bert.pt  \n","  inflating: ./bert_data/cnndm.train.37.bert.pt  \n","  inflating: ./bert_data/cnndm.train.38.bert.pt  \n","  inflating: ./bert_data/cnndm.train.39.bert.pt  \n","  inflating: ./bert_data/cnndm.train.3.bert.pt  \n","  inflating: ./bert_data/cnndm.train.40.bert.pt  \n","  inflating: ./bert_data/cnndm.train.41.bert.pt  \n","  inflating: ./bert_data/cnndm.train.42.bert.pt  \n","  inflating: ./bert_data/cnndm.train.43.bert.pt  \n","  inflating: ./bert_data/cnndm.train.44.bert.pt  \n","  inflating: ./bert_data/cnndm.train.45.bert.pt  \n","  inflating: ./bert_data/cnndm.train.46.bert.pt  \n","  inflating: ./bert_data/cnndm.train.47.bert.pt  \n","  inflating: ./bert_data/cnndm.train.48.bert.pt  \n","  inflating: ./bert_data/cnndm.train.49.bert.pt  \n","  inflating: ./bert_data/cnndm.train.4.bert.pt  \n","  inflating: ./bert_data/cnndm.train.50.bert.pt  \n","  inflating: ./bert_data/cnndm.train.51.bert.pt  \n","  inflating: ./bert_data/cnndm.train.52.bert.pt  \n","  inflating: ./bert_data/cnndm.train.53.bert.pt  \n","  inflating: ./bert_data/cnndm.train.54.bert.pt  \n","  inflating: ./bert_data/cnndm.train.55.bert.pt  \n","  inflating: ./bert_data/cnndm.train.56.bert.pt  \n","  inflating: ./bert_data/cnndm.train.57.bert.pt  \n","  inflating: ./bert_data/cnndm.train.58.bert.pt  \n","  inflating: ./bert_data/cnndm.train.59.bert.pt  \n","  inflating: ./bert_data/cnndm.train.5.bert.pt  \n","  inflating: ./bert_data/cnndm.train.60.bert.pt  \n","  inflating: ./bert_data/cnndm.train.61.bert.pt  \n","  inflating: ./bert_data/cnndm.train.62.bert.pt  \n","  inflating: ./bert_data/cnndm.train.63.bert.pt  \n","  inflating: ./bert_data/cnndm.train.64.bert.pt  \n","  inflating: ./bert_data/cnndm.train.65.bert.pt  \n","  inflating: ./bert_data/cnndm.train.66.bert.pt  \n","  inflating: ./bert_data/cnndm.train.67.bert.pt  \n","  inflating: ./bert_data/cnndm.train.68.bert.pt  \n","  inflating: ./bert_data/cnndm.train.69.bert.pt  \n","  inflating: ./bert_data/cnndm.train.6.bert.pt  \n","  inflating: ./bert_data/cnndm.train.70.bert.pt  \n","  inflating: ./bert_data/cnndm.train.71.bert.pt  \n","  inflating: ./bert_data/cnndm.train.72.bert.pt  \n","  inflating: ./bert_data/cnndm.train.73.bert.pt  \n","  inflating: ./bert_data/cnndm.train.74.bert.pt  \n","  inflating: ./bert_data/cnndm.train.75.bert.pt  \n","  inflating: ./bert_data/cnndm.train.76.bert.pt  \n","  inflating: ./bert_data/cnndm.train.77.bert.pt  \n","  inflating: ./bert_data/cnndm.train.78.bert.pt  \n","  inflating: ./bert_data/cnndm.train.79.bert.pt  \n","  inflating: ./bert_data/cnndm.train.7.bert.pt  \n","  inflating: ./bert_data/cnndm.train.80.bert.pt  \n","  inflating: ./bert_data/cnndm.train.81.bert.pt  \n","  inflating: ./bert_data/cnndm.train.82.bert.pt  \n","  inflating: ./bert_data/cnndm.train.83.bert.pt  \n","  inflating: ./bert_data/cnndm.train.84.bert.pt  \n","  inflating: ./bert_data/cnndm.train.85.bert.pt  \n","  inflating: ./bert_data/cnndm.train.86.bert.pt  \n","  inflating: ./bert_data/cnndm.train.87.bert.pt  \n","  inflating: ./bert_data/cnndm.train.88.bert.pt  \n","  inflating: ./bert_data/cnndm.train.89.bert.pt  \n","  inflating: ./bert_data/cnndm.train.8.bert.pt  \n","  inflating: ./bert_data/cnndm.train.90.bert.pt  \n","  inflating: ./bert_data/cnndm.train.91.bert.pt  \n","  inflating: ./bert_data/cnndm.train.92.bert.pt  \n","  inflating: ./bert_data/cnndm.train.93.bert.pt  \n","  inflating: ./bert_data/cnndm.train.94.bert.pt  \n","  inflating: ./bert_data/cnndm.train.95.bert.pt  \n","  inflating: ./bert_data/cnndm.train.96.bert.pt  \n","  inflating: ./bert_data/cnndm.train.97.bert.pt  \n","  inflating: ./bert_data/cnndm.train.98.bert.pt  \n","  inflating: ./bert_data/cnndm.train.99.bert.pt  \n","  inflating: ./bert_data/cnndm.train.9.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.0.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.1.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.2.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.3.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.4.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.5.bert.pt  \n","  inflating: ./bert_data/cnndm.valid.6.bert.pt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w8rHG49yedkF","executionInfo":{"status":"ok","timestamp":1604768884370,"user_tz":-180,"elapsed":840,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"78d5f841-dfbd-4021-8315-067339f736b5","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd src"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/content/BertSum/src\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jHqhIu_LsNeG"},"source":["Пример входных данных"]},{"cell_type":"code","metadata":{"id":"TLDitZ3mnv_J","executionInfo":{"status":"ok","timestamp":1604768886968,"user_tz":-180,"elapsed":1039,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import torch\n","cnn_test_samp = torch.load(\"/content/BertSum/bert_data/cnndm.test.0.bert.pt\")"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"jR9kyPftqoWv","executionInfo":{"status":"ok","timestamp":1604768889427,"user_tz":-180,"elapsed":1106,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["cnn_test_samp0 = cnn_test_samp[0]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lx4X9QDEq_uX","executionInfo":{"status":"ok","timestamp":1604768892000,"user_tz":-180,"elapsed":870,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"161f4172-bf69-4844-b191-58aecaf2262e","colab":{"base_uri":"https://localhost:8080/"}},"source":["cnn_test_samp0.keys()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"RydoDsS6oQ6g","executionInfo":{"status":"ok","timestamp":1604768899117,"user_tz":-180,"elapsed":921,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"3ebb379a-7a0a-4840-cb1a-7eb9458275ef","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(cnn_test_samp0['clss']) # индексы CLS токенов для предложений входного текста \n","print(cnn_test_samp0['labels']) # таргет метки для предложений (1 - входит в summary, 0 - не входит)\n","print(cnn_test_samp0['segs']) # id сегментов предложений \n","print(cnn_test_samp0['src']) # id слов"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[0, 25, 57, 78, 112, 136, 174, 197, 223, 245, 285, 301, 337, 358, 382, 416, 452]\n","[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[101, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 2886, 1999, 4199, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 2018, 2069, 2074, 3369, 2005, 1037, 13609, 2565, 1999, 3304, 2043, 1996, 5043, 3047, 1999, 2254, 1012, 102, 101, 2002, 2001, 10583, 2067, 2000, 3190, 3081, 2250, 10771, 2006, 2233, 2322, 1010, 2021, 2002, 2351, 2006, 4465, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 102, 101, 2002, 2001, 2579, 2000, 1037, 2966, 4322, 1999, 1996, 3190, 2181, 1010, 2485, 2000, 2010, 2155, 2188, 1999, 8904, 3449, 9644, 1012, 102, 101, 2002, 2351, 2006, 4465, 2012, 7855, 3986, 2902, 1011, 2966, 19684, 1005, 1055, 2436, 14056, 3581, 18454, 6199, 2319, 2758, 1037, 3426, 1997, 2331, 24185, 1050, 1005, 1056, 2022, 2207, 2127, 6928, 2012, 1996, 5700, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 1012, 102, 101, 2006, 4465, 1010, 2010, 5542, 9460, 2626, 3784, 1024, 1036, 2023, 2851, 2026, 5542, 4080, 1005, 1055, 3969, 2001, 4196, 2039, 2000, 6014, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 102, 101, 1036, 2012, 1996, 2927, 1997, 2254, 2002, 2253, 2000, 4199, 2000, 2817, 7548, 1998, 2006, 1996, 2126, 2188, 2013, 1037, 2283, 2002, 2001, 23197, 4457, 1998, 6908, 2125, 1037, 2871, 6199, 2958, 1998, 2718, 1996, 5509, 2917, 1012, 102, 101, 1036, 2002, 2001, 1999, 1037, 16571, 1998, 1999, 4187, 4650, 2005, 2706, 1012, 1005, 102, 101, 13723, 20073, 1010, 2040, 2056, 2016, 2003, 1037, 2485, 2155, 2767, 1010, 2409, 2026, 9282, 2166, 1010, 2008, 9587, 29076, 2018, 2069, 2042, 1999, 1996, 2406, 2005, 2416, 2847, 2043, 1996, 5043, 3047, 1012, 102, 101, 2016, 2056, 2002, 2001, 2001, 2894, 2012, 1996, 2051, 1997, 1996, 6884, 6101, 1998, 3167, 5167, 2020, 7376, 1012, 102, 101, 2016, 2794, 2008, 2002, 2001, 1999, 1037, 2512, 1011, 2966, 2135, 10572, 16571, 1010, 2383, 4265, 3809, 8985, 1998, 4722, 9524, 1012, 102, 101, 9587, 29076, 2001, 1037, 2353, 1011, 2095, 5446, 2350, 2013, 8904, 3449, 9644, 1010, 5665, 1012, 1010, 2040, 2001, 8019, 1999, 1037, 13609, 1011, 2146, 2565, 2012, 2198, 9298, 4140, 2118, 1012, 102, 101, 9587, 29076, 6272, 2000, 1996, 2082, 1005, 1055, 3127, 1997, 1996, 13201, 16371, 13577, 1010, 4311, 1996, 3190, 10969, 2040, 6866, 1037, 3696, 2648, 1037, 2311, 3752, 1036, 11839, 2005, 9587, 29076, 1012, 1005, 102, 101, 1996, 13577, 1005, 1055, 5947, 3127, 2623, 4465, 5027, 3081, 10474, 2008, 1037, 3986, 2326, 2097, 2022, 2218, 2006, 3721, 2000, 3342, 9587, 29076, 1012, 102]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xs7zKGfYrM0i","executionInfo":{"status":"ok","timestamp":1604768914196,"user_tz":-180,"elapsed":864,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"2470e961-5428-4bf1-f41a-4fa98068f9af","colab":{"base_uri":"https://localhost:8080/"}},"source":["cnn_test_samp0['src_txt'] # входной текст\n"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n"," 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n"," 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n"," 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n"," 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n"," \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n"," 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n"," \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n"," 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n"," '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n"," \"` he was in a coma and in critical condition for months . '\",\n"," 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n"," 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n"," 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n"," 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n"," \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n"," \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"-DSbhBlXrRWH","executionInfo":{"status":"ok","timestamp":1604768924898,"user_tz":-180,"elapsed":824,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"c9db2d94-815f-4514-c588-503ec311811b","colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["cnn_test_samp0['tgt_txt'] # target summary"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"othq3BNHfI33"},"source":["Обучение модели"]},{"cell_type":"code","metadata":{"id":"PW5EKEL8d9GG","executionInfo":{"status":"ok","timestamp":1604769719038,"user_tz":-180,"elapsed":24708,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"d5aabd48-238f-41af-ee4f-737b768b5f11","colab":{"base_uri":"https://localhost:8080/"}},"source":["!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 5000 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000"],"execution_count":38,"outputs":[{"output_type":"stream","text":["[2020-11-07 17:21:38,633 INFO] Device ID 0\n","[2020-11-07 17:21:38,633 INFO] Device cuda\n","[2020-11-07 17:21:38,674 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","[2020-11-07 17:21:38,675 INFO] extracting archive file ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpjfjnwr2p\n","[2020-11-07 17:21:42,603 INFO] Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","[2020-11-07 17:21:49,181 INFO] Summarizer(\n","  (bert): Bert(\n","    (model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): BertLayerNorm()\n","        (dropout): Dropout(p=0.1)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): BertLayerNorm()\n","                (dropout): Dropout(p=0.1)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (encoder): Classifier(\n","    (linear1): Linear(in_features=768, out_features=1, bias=True)\n","    (sigmoid): Sigmoid()\n","  )\n",")\n","gpu_rank 0\n","[2020-11-07 17:21:49,221 INFO] * number of parameters: 109483009\n","[2020-11-07 17:21:49,221 INFO] Start training...\n","[2020-11-07 17:21:49,321 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001\n","[2020-11-07 17:22:01,266 INFO] Loading train dataset from ../bert_data/cnndm.train.81.bert.pt, number of examples: 2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_DqPAkaWfI4B"},"source":["Тестирование на валидационных и тестовых данных"]},{"cell_type":"code","metadata":{"id":"TRHkfCKHe0d7","executionInfo":{"status":"ok","timestamp":1604769569392,"user_tz":-180,"elapsed":1846,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"11098945-6bc5-4ccb-d2d5-8294ec946eda","colab":{"base_uri":"https://localhost:8080/"}},"source":["!python train.py -mode validate -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier  -visible_gpus 0  -gpu_ranks 0 -batch_size 30000  -log_file ../logs/bert_classifier_valid  -result_path ../results/cnndm -test_all -block_trigram true"],"execution_count":35,"outputs":[{"output_type":"stream","text":["[2020-11-07 17:19:31,738 INFO] PPL []\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wD3hXFOMfI4M"},"source":["Примеры summary"]},{"cell_type":"code","metadata":{"id":"5mVQNPzaHBDZ","executionInfo":{"status":"error","timestamp":1604769594991,"user_tz":-180,"elapsed":995,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"28456ae4-1e94-409f-9330-a93d627b7c2a","colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["# extracted summary\n","N = 20\n","with open(\"/content/BertSum/results/cnndm_step5000.candidate\") as f:\n","    dec = [next(f) for x in range(N)]"],"execution_count":37,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-5bdcd2aefa2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extracted summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/BertSum/results/cnndm_step200.candidate\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/BertSum/results/cnndm_step200.candidate'"]}]},{"cell_type":"code","metadata":{"id":"ONnGsk_6YC8T"},"source":["# target summary\n","N = 20\n","with open(\"/content/BertSum/results/cnndm_step5000.gold\") as f:\n","    ref = [next(f) for x in range(N)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTUMy-7lTROZ","outputId":"bbe0ae3c-4cf3-4512-89b5-011b888a9c6e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["ref[0].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the 79th masters tournament gets underway at augusta national on thursday',\n"," 'rory mcilroy and tiger woods will be the star attractions in the field bidding for the green jacket at 2015 masters',\n"," 'mcilroy , justin rose , ian poulter , graeme mcdowell and more gave sportsmail the verdict on each hole at augusta',\n"," 'click on the brilliant interactive graphic below for details on each hole of the masters 2015 course',\n"," 'click here for all the latest news from the masters 2015\\n']"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"vs-rkv00TNma","outputId":"344aa303-e903-4a65-d5a3-cee14a889429","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["dec[0].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['to help get you in the mood for the first major of the year , rory mcilroy , ian poulter , graeme mcdowell and justin rose , plus past masters champions nick faldo and charl schwartzel , give the lowdown on every hole at the world-famous augusta national golf club .',\n"," 'the masters 2015 is almost here .',\n"," 'click on the graphic below to get a closer look at what the biggest names in the game will face when they tee off on thursday .\\n']"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"code","metadata":{"id":"QblpOaQIYdII","outputId":"5b8a0520-1b19-4c61-e717-d3c3998e632d","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["ref[1].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"jeff powell looks ahead to saturday 's fight at the mgm grand\",\n"," 'floyd mayweather takes on manny pacquiao in $ 300m showdown',\n"," 'both fighters arrived in las vegas on tuesday with public appearances',\n"," 'read : mayweather makes official arrival ahead of manny pacquiao fight',\n"," 'al haymon : the man behind mayweather who is revolutionising boxing',\n"," \"mayweather vs pacquiao takes centre stage ... but who 's on the undercard ?\\n\"]"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"-FpWNGk_U4E8","outputId":"664943ad-edd6-415e-8c8f-79eceb7e4a28","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["dec[1].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"sportsmail 's boxing correspondent jeff powell looks ahead to saturday 's mega-fight at the mgm grand after witnessing floyd mayweather and manny pacquiao 's grand arrivals in las vegas .\",\n"," 'both boxers made public appearances on tuesday as their $ 300million showdown draws ever closer , and our man powell was there .',\n"," \"powell reflects on the pair 's arrivals on the las vegas strip and looks forward to the rest of the week .\\n\"]"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"9ZCaWmqEYgLN","outputId":"c0381a73-6225-4669-f022-b3043f493651","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["ref[2].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['gary locke has been interim manager since start of february',\n"," 'locke has won two and drawn four of his seven games in charge',\n"," 'the 37-year-old took over when allan johnston quit\\n']"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"jMiC_NqjYpQS","outputId":"75ab376b-1744-4182-d756-1370496e6b3d","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["dec[2].split('<q>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['kilmarnock interim manager gary locke has been given the role on a permanent basis after signing a three-year deal .',\n"," 'the former hearts boss joined the club as assistant boss to allan johnston last summer but took control of the team when his ex-tynecastle team-mate quit at the start of february .',\n"," 'the 39-year-old - who will speak at a press conference on friday morning - has lost just once in seven games since taking over at rugby park .\\n']"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"mSkCIBD_YsaR"},"source":[""],"execution_count":null,"outputs":[]}]}