{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"sem4_language_models.ipynb","provenance":[],"collapsed_sections":["8gF119Ux1HaY","fC8XCdrZ1Hcg","1E1tAyYU1HeP"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"UnzqMx3W1HAe","colab_type":"text"},"source":["# Language modelling"]},{"cell_type":"markdown","metadata":{"id":"gzkNxXTj1HAu","colab_type":"text"},"source":["\n","Обучим две различные символьные модели для генерации динозавров:\n","* модель на символьных биграммах\n","* ***RNN***-модель.\n"]},{"cell_type":"markdown","metadata":{"id":"NHVEAXRs1HA6","colab_type":"text"},"source":["## Bigram model\n"]},{"cell_type":"code","metadata":{"id":"KtIM9SS81HBG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1600420423151,"user_tz":-180,"elapsed":2031,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"1778e44e-0300-48be-e08f-a76e8110986e"},"source":["!wget https://raw.githubusercontent.com/artemovae/NLP-seminar-LM/master/dinos.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-09-18 09:13:44--  https://raw.githubusercontent.com/artemovae/NLP-seminar-LM/master/dinos.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19909 (19K) [text/plain]\n","Saving to: ‘dinos.txt’\n","\n","dinos.txt           100%[===================>]  19.44K  --.-KB/s    in 0.007s  \n","\n","2020-09-18 09:13:45 (2.60 MB/s) - ‘dinos.txt’ saved [19909/19909]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c6iUdpaO1HCO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1600420428111,"user_tz":-180,"elapsed":1317,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"89da4b0f-d3cf-4d51-c52d-085d2419da55"},"source":["!cat dinos.txt | tail"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Zhuchengtyrannus\n","Ziapelta\n","Zigongosaurus\n","Zizhongosaurus\n","Zuniceratops\n","Zunityrannus\n","Zuolong\n","Zuoyunlong\n","Zupaysaurus\n","Zuul"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C8bDbP7V1HC_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1600420439474,"user_tz":-180,"elapsed":851,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"38f7964d-9b9d-4f13-f2bb-9fd31e1aee54"},"source":["names = ['<' + name.strip().lower() + '>' for name in open('dinos.txt').readlines()]\n","print(names[:10])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['<aachenosaurus>', '<aardonyx>', '<abdallahsaurus>', '<abelisaurus>', '<abrictosaurus>', '<abrosaurus>', '<abydosaurus>', '<acanthopholis>', '<achelousaurus>', '<acheroraptor>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KbECrMSs1HDn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420448409,"user_tz":-180,"elapsed":6694,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import nltk"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0FvBchI1HEI","colab_type":"text"},"source":["Вычислим частоту каждого символа в корпусе имен динозавров"]},{"cell_type":"code","metadata":{"id":"_rFk6yvK1HEN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420450453,"user_tz":-180,"elapsed":890,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["chars = [char for name in names for char in name]\n","freq = nltk.FreqDist(chars)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZY82ymMz1HEx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":531},"executionInfo":{"status":"ok","timestamp":1600420458932,"user_tz":-180,"elapsed":897,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"811da6b7-cbb4-4f07-82b4-1424f28021cd"},"source":["freq"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'<': 1536,\n","          '>': 1536,\n","          'a': 2487,\n","          'b': 171,\n","          'c': 539,\n","          'd': 341,\n","          'e': 913,\n","          'f': 37,\n","          'g': 360,\n","          'h': 548,\n","          'i': 944,\n","          'j': 55,\n","          'k': 141,\n","          'l': 617,\n","          'm': 328,\n","          'n': 1081,\n","          'o': 1710,\n","          'p': 552,\n","          'q': 23,\n","          'r': 1704,\n","          's': 2285,\n","          't': 852,\n","          'u': 2123,\n","          'v': 111,\n","          'w': 41,\n","          'x': 85,\n","          'y': 266,\n","          'z': 60})"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"2VnuL9Bf1HFS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600420464401,"user_tz":-180,"elapsed":834,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"72841960-9e3e-4e86-d53f-9d450dbc7846"},"source":["print(list(freq.keys()))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['<', 'a', 'c', 'h', 'e', 'n', 'o', 's', 'u', 'r', '>', 'd', 'y', 'x', 'b', 'l', 'i', 't', 'p', 'v', 'm', 'g', 'f', 'j', 'k', 'w', 'z', 'q']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TkHZnWFN1HFy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1600420467255,"user_tz":-180,"elapsed":848,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"c9535787-478a-46b5-b754-3469bcb6fd8f"},"source":["freq.most_common(10)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('a', 2487),\n"," ('s', 2285),\n"," ('u', 2123),\n"," ('o', 1710),\n"," ('r', 1704),\n"," ('<', 1536),\n"," ('>', 1536),\n"," ('n', 1081),\n"," ('i', 944),\n"," ('e', 913)]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"-CFqe1cB1HGP","colab_type":"text"},"source":["Определим функцию для вычисления условной вероятности символов"]},{"cell_type":"code","metadata":{"id":"3C4VP4hR1HGU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420475951,"user_tz":-180,"elapsed":762,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["l = sum([freq[char] for char in freq])\n","def unigram_prob(char):\n","    return freq[char] / l"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kiiv0vo1HG2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600420485780,"user_tz":-180,"elapsed":2758,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"e0ae5694-9ca1-4b0f-a9a0-ec6d11381a7a"},"source":["print('p(a) = %1.4f' %unigram_prob('a'))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["p(a) = 0.1160\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pbpj8VLv1HHX","colab_type":"text"},"source":["Вычислим условную вероятность каждого символа в зависимости от того, какой символ стоял на предыдущей позиции."]},{"cell_type":"code","metadata":{"id":"L-GW1NZB1HHd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420527155,"user_tz":-180,"elapsed":10260,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXvcbL1m1HId","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"status":"ok","timestamp":1600420532478,"user_tz":-180,"elapsed":807,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"6bba9267-ca4f-4e92-9bca-d15b8be465c9"},"source":["cfreq['a']"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'>': 138,\n","          'a': 11,\n","          'b': 24,\n","          'c': 100,\n","          'd': 36,\n","          'e': 42,\n","          'f': 6,\n","          'g': 40,\n","          'h': 17,\n","          'i': 23,\n","          'j': 5,\n","          'k': 20,\n","          'l': 138,\n","          'm': 68,\n","          'n': 347,\n","          'o': 22,\n","          'p': 89,\n","          'q': 3,\n","          'r': 124,\n","          's': 171,\n","          't': 204,\n","          'u': 791,\n","          'v': 30,\n","          'w': 6,\n","          'x': 12,\n","          'y': 12,\n","          'z': 8})"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"VuV5anvn1HJL","colab_type":"text"},"source":["Оценим условные вероятности с помощью MLE."]},{"cell_type":"code","metadata":{"id":"uWnDDdJO1HJQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420542337,"user_tz":-180,"elapsed":740,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-q-UONs1HJz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1600420544906,"user_tz":-180,"elapsed":937,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"eec47f4c-9a37-4521-aa1f-a7d2d25c897d"},"source":["print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n","print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n","print('p(a u) = %1.4f' %cprob['a'].prob('u'))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["p(a a) = 0.0044\n","p(a b) = 0.0097\n","p(a u) = 0.3181\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PmXTHNpU1HKQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1600420555644,"user_tz":-180,"elapsed":756,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"6351b2a4-14f1-45f1-e3bb-c543922a0f82"},"source":["cprob['a'].generate()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'u'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"M80v-kVK1HLO","colab_type":"text"},"source":["### Task 1.\n","a. Write a function to generate a dinosaur name of **fixed** length. Use '<' as a start of name symbol.\n","\n","b. Write a function to generate a dinosaur names of any length. "]},{"cell_type":"code","metadata":{"id":"h-ZmpvP71HLY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600420571338,"user_tz":-180,"elapsed":1040,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def generate_n_word(n=10):\n","    word = '<'    \n","    while (len(word) <= n):\n","        c = cprob[word[-1]].generate()\n","        if c not in {'<','>'}: word += c        \n","    word += '>'\n","    return word"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJqCN5LE1HLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1589303478691,"user_tz":-180,"elapsed":1213,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"8ebef226-d4a5-4fd0-8b70-524376a41c41"},"source":["generate_n_word()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<trdhyataba>'"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"dBBT-_pC5j24","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"In-qSTog5-Yz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1589304146705,"user_tz":-180,"elapsed":965,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"c0f4e8cf-7612-416d-9eb4-56d6a1b78470"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<ausatonimiusagonthapurusasaskiveishos>'"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"VpiHvyOwqGi_","colab_type":"text"},"source":["## Модели n-грамм для слов\n","(https://nlpforhackers.io/language-models/)"]},{"cell_type":"code","metadata":{"id":"h7w_ThviqHY7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1600423435768,"user_tz":-180,"elapsed":1788,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"73d6df29-e1ec-4252-9787-0429202948e5"},"source":["from nltk.corpus import reuters\n","from nltk import bigrams, trigrams\n","from collections import Counter, defaultdict\n","\n","nltk.download('reuters')\n","nltk.download('punkt')\n"," \n","first_sentence = reuters.sents()[0]\n","print (first_sentence) # [u'ASIAN', u'EXPORTERS', u'FEAR', u'DAMAGE', u'FROM' ...\n"," \n","# Get the bigrams\n","print (list(bigrams(first_sentence))) # [(u'ASIAN', u'EXPORTERS'), (u'EXPORTERS', u'FEAR'), (u'FEAR', u'DAMAGE'), (u'DAMAGE', u'FROM'), ...\n"," \n","# Get the padded bigrams\n","print (list(bigrams(first_sentence, pad_left=True, pad_right=True))) # [(None, u'ASIAN'), (u'ASIAN', u'EXPORTERS'), (u'EXPORTERS', u'FEAR'), (u'FEAR', u'DAMAGE'), (u'DAMAGE', u'FROM'),\n"," \n","# Get the trigrams\n","print (list(trigrams(first_sentence))) # [(u'ASIAN', u'EXPORTERS', u'FEAR'), (u'EXPORTERS', u'FEAR', u'DAMAGE'), (u'FEAR', u'DAMAGE', u'FROM'), ...\n"," \n","# Get the padded trigrams\n","print (list(trigrams(first_sentence, pad_left=True, pad_right=True))) # [(None, None, u'ASIAN'), (None, u'ASIAN', u'EXPORTERS'), (u'ASIAN', u'EXPORTERS', u'FEAR'), (u'EXPORTERS', u'FEAR', u'DAMAGE'), (u'FEAR', u'DAMAGE', u'FROM') ..."],"execution_count":56,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n","[nltk_data]   Package reuters is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n","[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n","[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n","[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n","[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KW3O6-LcrGYp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1600423534061,"user_tz":-180,"elapsed":12340,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"340e5d7f-6b10-42b3-dba0-aa33cb2cb3ff"},"source":["model = defaultdict(lambda: defaultdict(lambda: 0))\n"," \n","for sentence in reuters.sents():\n","    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n","        model[(w1, w2)][w3] += 1\n"," \n"," \n","print (model[\"what\", \"the\"][\"economists\"]) # \"economists\" follows \"what the\" 2 times\n","print (model[\"what\", \"the\"][\"nonexistingword\"]) # 0 times\n","print (model[None, None][\"The\"]) # 8839 sentences start with \"The\"\n"," \n","# Let's transform the counts to probabilities\n","for w1_w2 in model:\n","    total_count = float(sum(model[w1_w2].values()))\n","    for w3 in model[w1_w2]:\n","        model[w1_w2][w3] /= total_count\n"," \n","print (model[\"what\", \"the\"][\"economists\"]) # 0.0434782608696\n","print (model[\"what\", \"the\"][\"nonexistingword\"]) # 0.0\n","print (model[None, None][\"The\"]) # 0.161543241465\n"," "],"execution_count":58,"outputs":[{"output_type":"stream","text":["2\n","0\n","8839\n","0.043478260869565216\n","0.0\n","0.16154324146501936\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f8X-KSIfr1J8","colab_type":"text"},"source":["The probability of a sequence is computed using conditional probabilities.\n","The probability of word[i] given word[i-1] and word[i-2] is P(word[i] | word[i-1], word[i-2]) which in our case is equal to: model[(word[i-2], word[i-1])][word[i]]\n","\n","Let’s add the probability computation in the generation script:"]},{"cell_type":"code","metadata":{"id":"0hxRPdLhrVKb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1600423665730,"user_tz":-180,"elapsed":845,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"9caac51e-a049-4605-bb9c-55a88ebc93d5"},"source":["import random\n"," \n"," \n","text = [None, None]\n","prob = 1.0  # <- Init probability\n"," \n","sentence_finished = False\n"," \n","while not sentence_finished:\n","    r = random.random()\n","    accumulator = .0\n"," \n","    for word in model[tuple(text[-2:])].keys():\n","        accumulator += model[tuple(text[-2:])][word]\n"," \n","        if accumulator >= r:\n","            prob *= model[tuple(text[-2:])][word]  # <- Update the probability with the conditional probability of the new word\n","            text.append(word)\n","            break\n"," \n","    if text[-2:] == [None, None]:\n","        sentence_finished = True\n"," \n","print (\"Probability of text=\", prob)  # <- Print the probability of the text\n","print (' '.join([t for t in text if t]))"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Probability of text= 1.09883285553263e-67\n","Iranian Prime Minister Yasuhiro Nakasone leaves for Washington , told the Senate Reagan ' s could be created mainly in speciality chemicals unit , Gelco Express , contributing about 20 pct stake in privatised companies but can boost their prime to a satisfactory offer by TransCanada would own 20 pct stake in Silicon Systems Inc > said it might make it more resilient against exchange rate stability around current levels , it has held talks with Washington which officials described as generally unsatisfactory , trade sources said .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OB6SH5vhsPor","colab_type":"text"},"source":["##Task 2"]},{"cell_type":"markdown","metadata":{"id":"zVSWTkQqsaZv","colab_type":"text"},"source":["Напишите функцию, которая будет генерировать предложение и оценивать его вероятность. Подсказка: предложение должно заканчиваться соответствующим знаком препинания."]},{"cell_type":"code","metadata":{"id":"4LY1C-_-sjDK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iz0QMOdD1HMK","colab_type":"text"},"source":["## Реккурентные нейронные сети (RNN)\n","\n","Исходная последовательность:\n","\n","$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n","\n","Для каждого входного значения $x_{1:i}$ получаем на выходе $y_i$:\n","\n","$y_i = RNN(x_{1:i})$, $y_i \\in \\mathbb{R}^{d_{out}}$\n","\n","Для всей последовательности $x_{1:n}$:\n","\n","$y_{1:n} = RNN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$"]},{"cell_type":"markdown","metadata":{"id":"O34g-rG21HMN","colab_type":"text"},"source":["$R$ - рекурсивная функция активации, зависящая от двух параметров: $x_i$ и $s_{i-1}$ (вектор предыдущего состояния)\n","\n","$RNN^{*}(x_{1:n}, s_0) = y_{1:n}$\n","\n","$y_i = O(s_i) = g(W^{out}[s_{i} ,x_i] +b)$\n","\n","$s_i = R(s_{i-1}, x_i)$\n","\n","$s_i = R(s_{i-1}, x_i) = g(W^{hid}[s_{i-1} ,x_i] +b)$  -- конкатенация $[s_{i-1}, x]$\n","\n","$x_i \\in \\mathbb{R}^{d_{in}}$, $y_i \\in \\mathbb{R}^{ d_{out}}$, $s_i \\in \\mathbb{R}^{d_{hid}}$\n","\n","$W^{hid} \\in \\mathbb{R}^{(d_{in}+d_{out}) \\times d_{hid}}$, $W^{out} \\in \\mathbb{R}^{d_{hid} \\times d_{out}}$"]},{"cell_type":"markdown","metadata":{"id":"RI9dlfCK1HMU","colab_type":"text"},"source":["Построим языковую модель на основе RNN с помощью pytorch"]},{"cell_type":"code","metadata":{"id":"gclyau4j1HMc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600424982053,"user_tz":-180,"elapsed":3474,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pdb\n","from torch.utils.data import Dataset, DataLoader\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","torch.set_printoptions(linewidth=200)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVk_vofO1HM0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600424983927,"user_tz":-180,"elapsed":947,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","hidden_size = 50"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FF4Dp1OK1HNS","colab_type":"text"},"source":["Подготовим данные"]},{"cell_type":"code","metadata":{"code_folding":[0],"id":"Q49llCoq1HNV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600424990542,"user_tz":-180,"elapsed":848,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["class DinosDataset(Dataset):\n","    def __init__(self):\n","        super().__init__()\n","        with open('dinos.txt') as f:\n","            content = f.read().lower()\n","            self.vocab = sorted(set(content)) + ['<', '>']\n","            self.vocab_size = len(self.vocab)\n","            self.lines = content.splitlines()\n","        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n","        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n","    \n","    def __getitem__(self, index):\n","        line = self.lines[index]\n","        #teacher forcing\n","        x_str = '<' + line \n","        y_str = line + '>' \n","        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n","        y = torch.empty(len(x_str), dtype=torch.long)\n","        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n","            x[i][self.ch_to_idx[x_ch]] = 1\n","            y[i] = self.ch_to_idx[y_ch]\n","        \n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.lines)"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xCx68KK1HNq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600424996847,"user_tz":-180,"elapsed":818,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["trn_ds = DinosDataset()\n","trn_dl = DataLoader(trn_ds, shuffle=True)"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4GHhNw21HOG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1600424998461,"user_tz":-180,"elapsed":591,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"335a0095-ad39-435d-dd4a-f2e103d948e0"},"source":["trn_ds.lines[1]"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'aardonyx'"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"caYYHf741HOh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1600425001978,"user_tz":-180,"elapsed":1031,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"72e3f57d-1fb4-4ce6-9cdc-c998015ea614"},"source":["print(trn_ds.idx_to_ch)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '<', 28: '>'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jhngI6P41HO0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425004426,"user_tz":-180,"elapsed":1000,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"cfa91d0d-912c-4cb4-bdb4-cbfb17bd8360"},"source":["trn_ds.vocab_size"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"heT6Yt1c1HPL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425009965,"user_tz":-180,"elapsed":1631,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["x, y = trn_ds[1]"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PVidOvMz1HPl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425011361,"user_tz":-180,"elapsed":625,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"b70b86fe-be80-48d0-9dd9-483ff0b83dbd"},"source":["x.shape"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([9, 29])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"dOoyQP661HP_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425013910,"user_tz":-180,"elapsed":875,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"e33bda86-993e-4314-b383-87fcf548fb5d"},"source":["y.shape"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([9])"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"nnDCqfhW1HQP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425016282,"user_tz":-180,"elapsed":809,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"f58e2ab9-0213-4b10-a1b4-965c7e11627a"},"source":["y"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1,  1, 18,  4, 15, 14, 25, 24, 28])"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"Zg4vQkSJ1HQo","colab_type":"text"},"source":["Опишем модель, функцию потерь и алгоритм оптимизации"]},{"cell_type":"code","metadata":{"code_folding":[],"id":"fiFev9H71HQs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425019887,"user_tz":-180,"elapsed":969,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.dropout = nn.Dropout(0.3)\n","        self.i2o = nn.Linear(hidden_size, output_size)\n","    \n","    def forward(self, h_prev, x):\n","        combined = torch.cat([h_prev, x], dim = 1) # concatenate x and h\n","        h = torch.tanh(self.dropout(self.i2h(combined)))\n","        y = self.i2o(h)\n","        return h, y"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bGJs5pv1HRD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425024979,"user_tz":-180,"elapsed":767,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=1e-2)"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tq9c9Gky1HRk","colab_type":"text"},"source":["![rnn](images/dinos3.png)"]},{"cell_type":"code","metadata":{"code_folding":[],"id":"udihoEbb1HRq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425028450,"user_tz":-180,"elapsed":947,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def sample(model):\n","    model.eval()\n","    word_size=0\n","    newline_idx = trn_ds.ch_to_idx['>']\n","    with torch.no_grad():\n","        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n","        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n","        start_char_idx = trn_ds.ch_to_idx['<']\n","        indices = [start_char_idx]\n","        x[0, start_char_idx] = 1\n","        predicted_char_idx = start_char_idx\n","        \n","        while predicted_char_idx != newline_idx and word_size != 50:\n","            h_prev, y_pred = model(h_prev, x)\n","            y_softmax_scores = torch.softmax(y_pred, dim=1)\n","            \n","            np.random.seed(np.random.randint(1, 5000))\n","            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n","            indices.append(idx)\n","            \n","            x = (y_pred == y_pred.max(1)[0]).float()\n","            \n","            predicted_char_idx = idx\n","            \n","            word_size += 1\n","        \n","        if word_size == 50:\n","            indices.append(newline_idx)\n","    return indices"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[],"id":"Xvv2iWX21HR9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425043611,"user_tz":-180,"elapsed":933,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def print_sample(sample_idxs):\n","    [print(trn_ds.idx_to_ch[x], end ='') for x in sample_idxs]\n","    print()"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PGCHFxl91HSY","colab_type":"text"},"source":["Обучим получившуюся модель"]},{"cell_type":"code","metadata":{"code_folding":[],"id":"Mi3I-sOd1HSc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425045857,"user_tz":-180,"elapsed":869,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def train_one_epoch(model, loss_fn, optimizer):\n","    model.train()\n","    for line_num, (x, y) in enumerate(trn_dl):\n","        loss = 0\n","        optimizer.zero_grad()\n","        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n","        x, y = x.to(device), y.to(device)\n","        for i in range(x.shape[1]):\n","            h_prev, y_pred = model(h_prev, x[:, i])\n","            loss += loss_fn(y_pred, y[:, i])\n","            \n","        if (line_num+1) % 100 == 0:\n","            print_sample(sample(model))\n","        loss.backward()\n","        optimizer.step()"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[],"id":"-UYREWWS1HSu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425051908,"user_tz":-180,"elapsed":976,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n","    for e in range(1, epochs+1):\n","        print('Epoch:{}'.format(e))\n","        train_one_epoch(model, loss_fn, optimizer)\n","        print()"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ydIB8Xi41HS8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600425119830,"user_tz":-180,"elapsed":63298,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"2aeedec1-925c-47ea-8095-77aaa87ffddb"},"source":["train(model, loss_fn, optimizer, epochs = 10)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Epoch:1\n","<keiha>\n","<btkuraiausus>\n","<srrgsqscup>\n","<ypvcspurus>\n","<lyros>\n","<tarocaupus>\n","<aersa>\n","<cunuc>\n","<anuptas>\n","<sniosaxrus>\n","<tartaeumus>\n","<aaoraurus>\n","<auaivosaurus>\n","<sintsrauris>\n","<alrashsaurug>\n","\n","Epoch:2\n","<curoa>\n","<aaurug>\n","<turisus>\n","<snosuaaurus>\n","<slsacomcaurus>\n","<tcrvsusrus>\n","<diurysaurur>\n","<jblpctusos>\n","<lardsaurus>\n","<xcuduuronaurxs>\n","<pusadnansaurun>\n","<scaresaurus>\n","<wrtesturus>\n","<kysmsiurus>\n","<eubuntancurus>\n","\n","Epoch:3\n","<fnbuc>\n","<xotbostpios>\n","<ytiorscaurusous>\n","<aanibjesaurus>\n","<woucosopaurus>\n","<irotarras>\n","<aupucauris>\n","<saucuacurus>\n","<ttpesgusus>\n","<ruankar>\n","<arguaasaerus>\n","<hctanvosaurus>\n","<qtmyspos>\n","<aenbnsaurus>\n","<searataurus>\n","\n","Epoch:4\n","<xrtasosaurus>\n","<snorsasacrus>\n","<euadooarlunoo>\n","<anwouesaurus>\n","<lyurosllrus>\n","<sctmrcauras>\n","<sancodaurus>\n","<strishurus>\n","<suarocaurus>\n","<afrpapaurus>\n","<anwoolosaurus>\n","<yulosocaurus>\n","<slvcdtsaurus>\n","<atcnvs>\n","<cturipaurus>\n","\n","Epoch:5\n","<roucisaurus>\n","<ucgunaurus>\n","<atantaurus>\n","<llrsnuurus>\n","<aasgisaurus>\n","<sibohpuros>\n","<jrotgosaurus>\n","<zrmossasaurus>\n","<eucnusaogsaumus>\n","<wiuctssnpaurus>\n","<rpuaipaurus>\n","<tcdsnasaurus>\n","<antotaurus>\n","<spnyosaurus>\n","<alsbprras>\n","\n","Epoch:6\n","<mamatgolaurus>\n","<crrtneaurus>\n","<ptubisaurus>\n","<talrobaurus>\n","<scgyntascurus>\n","<jxtlptocaurus>\n","<slsbasaerus>\n","<ecsahysaurus>\n","<dttizsurus>\n","<kicrocaurus>\n","<langonaqrus>\n","<pucstorastos>\n","<lpusapaurus>\n","<etbcrnasaurus>\n","<antotaurus>\n","\n","Epoch:7\n","<rtrystusus>\n","<maltbop>\n","<agpnapaurus>\n","<anyotesaurus>\n","<jwsioooasaurus>\n","<euacoocaurus>\n","<tcevotaurus>\n","<spnyrpaurus>\n","<aeubojsaurus>\n","<cuelasaurus>\n","<spuresaurus>\n","<stbnocaurus>\n","<afqoarapton>\n","<amuntascurus>\n","<hwrnkuras>\n","\n","Epoch:8\n","<aauous>\n","<aepiaaurus>\n","<sansovaurus>\n","<rsiyopauris>\n","<altasfurus>\n","<andsngcaurus>\n","<brrtmisaurus>\n","<subrochon>\n","<grabonaaurus>\n","<scgyotas>\n","<silsonvtops>\n","<amicgrdaurus>\n","<mamdsgndpurus>\n","<cttonaurus>\n","<irptaeodaurus>\n","\n","Epoch:9\n","<afrua>\n","<ctnrasaurus>\n","<supoisonuauruscuros>\n","<aojtcapter>\n","<tenaucaurus>\n","<srkeosaurus>\n","<taneccsaurus>\n","<shcierosaurus>\n","<tarsooeratrps>\n","<stbgocgua>\n","<gtadsmcaurus>\n","<sciynvdssaurus>\n","<yrgostapaurus>\n","<fubcsierus>\n","<harcktaurus>\n","\n","Epoch:10\n","<kljonzsaurus>\n","<malratetaurus>\n","<ctghcscaurus>\n","<ssiersotaurus>\n","<incisasaurus>\n","<ambshnaurus>\n","<vapttopaurus>\n","<rrubopaurus>\n","<taepnasaurus>\n","<aisstnrus>\n","<dslivosauros>\n","<amtasaurus>\n","<ahlropauiaurus>\n","<ttikooprurus>\n","<amjcgsaurus>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2sseocQG1HTN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425129963,"user_tz":-180,"elapsed":807,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def generate_dino(n=50):\n","    word = '<'\n","    for i in range(n):\n","        word += cprob[word[-1]].generate()\n","        if word[-1] == '>':\n","            return word\n","    word += '>'\n","    return word"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rqU5Mxh1HTg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425132406,"user_tz":-180,"elapsed":902,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"a40ed1b1-cdb0-4ad1-d0d8-53f016fc54bd"},"source":["nn.RNN"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.nn.modules.rnn.RNN"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"A-dctLty1HT0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1600425149104,"user_tz":-180,"elapsed":10901,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"f99d4cf2-532d-40f8-8cbd-40d9e57ce8a6"},"source":["for i in range(10):\n","    print(generate_dino())"],"execution_count":83,"outputs":[{"output_type":"stream","text":["<tonos>\n","<tros>\n","<bauaurosangaurosanaustops>\n","<auralanahops>\n","<elicomodrusas>\n","<fausatomeis>\n","<s>\n","<phigan>\n","<lbiosagagusabaurus>\n","<n>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tMVLZtS31HUF","colab_type":"text"},"source":["### Task 2.\n","Rewrite the sampling function to generate pangrams (words that contain each character of the alphabet only once)\n","\n","### Task 3.\n","Rewrite the sampling function so that it would be possible to change the sampling temperature\n","\n","### Task 4.\n","Implement the beam search for sampling"]},{"cell_type":"code","metadata":{"id":"n7J29hyD1HUI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"status":"error","timestamp":1600425166523,"user_tz":-180,"elapsed":854,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"31a4c083-4240-4407-b864-7db09222e448"},"source":["#a = y_softmax_scores.cpu().numpy().ravel()"],"execution_count":84,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-8ec5736433c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_softmax_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y_softmax_scores' is not defined"]}]},{"cell_type":"code","metadata":{"id":"cESnEczm1HUX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425175527,"user_tz":-180,"elapsed":796,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def equalize_probs_sqrt(in_vector):\n","    out_vector = np.zeros_like(in_vector)\n","    for i, el in enumerate(in_vector):\n","        out_vector[i] = np.math.sqrt(el)\n","\n","    return out_vector / sum(out_vector)"],"execution_count":85,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSSLWGYf1HUn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"status":"error","timestamp":1600425178280,"user_tz":-180,"elapsed":790,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"15a34e20-e716-457e-a77a-cc4ee2aefc8c"},"source":["#equalize_probs_sqrt(vec)"],"execution_count":86,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-6b8ba31090e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequalize_probs_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'vec' is not defined"]}]},{"cell_type":"code","metadata":{"id":"gYY7T0PI1HU3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600425286130,"user_tz":-180,"elapsed":860,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["def sample_hotter(model):\n","    model.eval()\n","    word_size=0\n","    newline_idx = trn_ds.ch_to_idx['>']\n","    with torch.no_grad():\n","        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n","        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n","        start_char_idx = trn_ds.ch_to_idx['<']\n","        indices = [start_char_idx]\n","        x[0, start_char_idx] = 1\n","        predicted_char_idx = start_char_idx\n","        \n","        while predicted_char_idx != newline_idx and word_size != 50:\n","            h_prev, y_pred = model(h_prev, x)\n","            y_softmax_scores = torch.softmax(y_pred, dim=1)\n","            \n","            np.random.seed(np.random.randint(1, 5000))\n","            \n","            next_prob_vector = equalize_probs_sqrt(y_softmax_scores.cpu().numpy().ravel())\n","            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=next_prob_vector)\n","            indices.append(idx)\n","            \n","            x = (y_pred == y_pred.max(1)[0]).float()\n","            \n","            predicted_char_idx = idx\n","            \n","            word_size += 1\n","        \n","        if word_size == 50:\n","            indices.append(newline_idx)\n","    return indices"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbgAzOPH1HVM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600425293979,"user_tz":-180,"elapsed":1088,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"694159c5-d83c-4382-e1a6-fd5182c1f022"},"source":["print_sample(sample_hotter(model))"],"execution_count":88,"outputs":[{"output_type":"stream","text":["<tnalethnauctyp>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sJ84UeWS1HVc","colab_type":"text"},"source":["# Reference\n","\n","1. Sampling in  RNN: https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n","2. Coursera course (main source): https://github.com/furkanu/deeplearning.ai-pytorch/tree/master/5-%20Sequence%20Models\n","3. Coursera course (main source): https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb\n","4. LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"]},{"cell_type":"markdown","metadata":{"id":"RpJ5oOw71HVf","colab_type":"text"},"source":["## Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"CN2y-8bg1HVi","colab_type":"text"},"source":["\n","#### Постановка задачи «sequence labeling»:\n"]},{"cell_type":"markdown","metadata":{"id":"hiuhtx931HVm","colab_type":"text"},"source":["\n","* Дан корпус текстов $D$\n","* Каждый текст представляет собой последовательность токенов\n","* Каждому токену присвоена метка из некоторого множества $V$"]},{"cell_type":"markdown","metadata":{"id":"2itmS0jz1HVn","colab_type":"text"},"source":["В зависимости от множества меток $V$ получаем разные типы подзадач. Например:\n","* если $V$ - множество частей речи, то это задача ***POS***-теггинга\n","* если $V$ - множество типов именованных сущностей, то это задача ***NER***"]},{"cell_type":"markdown","metadata":{"id":"9ojQmOTw1HVq","colab_type":"text"},"source":["Именованная сущность - любой фрагмент текста, обозначающий некоторый интересный объект."]},{"cell_type":"markdown","metadata":{"id":"7tCKeBrf1HVs","colab_type":"text"},"source":["#### Conditional Random Fields"]},{"cell_type":"markdown","metadata":{"id":"KVlxNw7U1HVv","colab_type":"text"},"source":["***Conditional Random Field*** - развитие метода Марковских случайных полей. Графовая модель, которая используется для представления совместных распределений набора нескольких случайных переменных. "]},{"cell_type":"markdown","metadata":{"id":"s51FX51t1HVz","colab_type":"text"},"source":["Особенности модели ***CRF***:"]},{"cell_type":"markdown","metadata":{"id":"79S68rHZ1HV2","colab_type":"text"},"source":["* Качество сильно зависит от выбора признаков\n","\n","* Один из лучших методов для ***NER*** и ***POS***-теггинга\n","\n","* Долго обучается\n","\n","* Хорошо работает в связке с рекуррентными нейросетями, моделирует совместное распределение на всей последовательности выходов сети одновременно"]},{"cell_type":"markdown","metadata":{"id":"dslKEuEc1HV5","colab_type":"text"},"source":["#### Архитектура BiLSTM-CRF"]},{"cell_type":"markdown","metadata":{"id":"l82FscQH1HV8","colab_type":"text"},"source":["В данной модели для каждого слова вычисляется его векторное представление на основе символьного состава слова, предобученных векоторных представлений (***Word2Vec***, ***FastText***, ***GloVe***), а также других признаков (***POS***-тег, роль в предложении и т.д.)"]},{"cell_type":"markdown","metadata":{"id":"iwiDECZ91HWA","colab_type":"text"},"source":["![representation](images/word_representation_model.png)"]},{"cell_type":"markdown","metadata":{"id":"8bQYkj1d1HWH","colab_type":"text"},"source":["Общая схема модели"]},{"cell_type":"markdown","metadata":{"id":"5SYOwhoY1HWL","colab_type":"text"},"source":["![bilstm_crf](images/bilstm_crf_model.png)"]},{"cell_type":"markdown","metadata":{"id":"U4_c_wQN1HWO","colab_type":"text"},"source":["Основные шаги алгоритма:\n","* Получить предобученные эмбеддинги слов коллекции (***word2vec***, ***GloVe***)\n","$$$$\n","* Обучить символьные эмбеддинги (***char-BiLSTM***, ***char-CNN***)\n","$$$$\n","* Составить для каждого слова морфологические/синтаксические признаки (***POS***-тег, роль в предложении и т.п.)\n","$$$$\n","* Объединить всё это и подать на вход основной сети (***BiLSTM***)\n","$$$$\n","* Выходы $h_t$ для всех слов предложения подавать на вход классификатору,\n","который будет предсказывать NER-тег (***SoftMax***, ***CRF***)"]},{"cell_type":"code","metadata":{"id":"FoCRtqo-1HWT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422042035,"user_tz":-180,"elapsed":723,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["#!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"zv-8Qbwv1HWj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1600422049713,"user_tz":-180,"elapsed":4013,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"4eade1f9-0b51-40ed-e197-b0011f42db9a"},"source":["!pip3 install torchvision"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4YIOIGvb1HW4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"ok","timestamp":1600422408547,"user_tz":-180,"elapsed":3184,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"abe9ada9-54ba-4817-c497-94f6e7d02e35"},"source":["!pip3 install torchtext"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7hXfLWz31HXJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":735},"executionInfo":{"status":"ok","timestamp":1600422430125,"user_tz":-180,"elapsed":13773,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"7d1d70b4-e2c2-4063-f263-4eea7c32795e"},"source":["!pip3 install natasha"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Collecting natasha\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/34/9abb6b5c95993001518e517f21157e2c955749ac4f3c79dc3c2cf25e72fe/natasha-1.3.0-py3-none-any.whl (34.4MB)\n","\u001b[K     |████████████████████████████████| 34.4MB 121kB/s \n","\u001b[?25hCollecting navec>=0.9.0\n","  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n","Collecting yargy>=0.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n","\u001b[?25hCollecting slovnet>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hCollecting pymorphy2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n","\u001b[?25hCollecting razdel>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n","Collecting ipymarkup>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec>=0.9.0->natasha) (1.18.5)\n","Collecting pymorphy2-dicts<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n","Collecting dawg-python>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Collecting intervaltree>=3\n","  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n","Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n","Building wheels for collected packages: intervaltree\n","  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=5a6ca6dc7cf94ece11fcfcd75011d9cd1d45b665b2e933d0b55a37f013f0deb4\n","  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n","Successfully built intervaltree\n","Installing collected packages: navec, pymorphy2-dicts, dawg-python, pymorphy2, yargy, razdel, slovnet, intervaltree, ipymarkup, natasha\n","  Found existing installation: intervaltree 2.1.0\n","    Uninstalling intervaltree-2.1.0:\n","      Successfully uninstalled intervaltree-2.1.0\n","Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B5RccXOW1HXa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1600422589628,"user_tz":-180,"elapsed":5181,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"74e4cfab-7ca4-44f3-df64-21016cedc4fc"},"source":["!python3 -m spacy download en_core_web_sm"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WuDGYuHN1HXs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1600422603390,"user_tz":-180,"elapsed":3427,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"fa1447a8-6c76-4ee7-f272-ee9c418f1410"},"source":["!pip install html5lib"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib) (1.15.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib) (0.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q9vMOimj1HX_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1600422653916,"user_tz":-180,"elapsed":1933,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"e4faf3d1-3e9c-48d5-c15b-bf85b43667ac"},"source":["!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa"],"execution_count":26,"outputs":[{"output_type":"stream","text":["--2020-09-18 09:50:55--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 827012 (808K) [text/plain]\n","Saving to: ‘eng.testa.1’\n","\n","\reng.testa.1           0%[                    ]       0  --.-KB/s               \reng.testa.1         100%[===================>] 807.63K  --.-KB/s    in 0.06s   \n","\n","2020-09-18 09:50:56 (13.1 MB/s) - ‘eng.testa.1’ saved [827012/827012]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pfVWhLo01HYV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1600422657247,"user_tz":-180,"elapsed":825,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"9b7250c5-1619-4a7d-dfbd-8a4a187cd19f"},"source":["!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"],"execution_count":27,"outputs":[{"output_type":"stream","text":["--2020-09-18 09:50:59--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 748096 (731K) [text/plain]\n","Saving to: ‘eng.testb’\n","\n","eng.testb           100%[===================>] 730.56K  --.-KB/s    in 0.05s   \n","\n","2020-09-18 09:50:59 (14.5 MB/s) - ‘eng.testb’ saved [748096/748096]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EmHqVBWu1HYk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1600422664874,"user_tz":-180,"elapsed":1047,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"9e7e5c96-e8ea-42af-b4f0-6ffa0fb0efb3"},"source":["!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train"],"execution_count":28,"outputs":[{"output_type":"stream","text":["--2020-09-18 09:51:06--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3281528 (3.1M) [text/plain]\n","Saving to: ‘eng.train’\n","\n","eng.train           100%[===================>]   3.13M  --.-KB/s    in 0.1s    \n","\n","2020-09-18 09:51:07 (23.3 MB/s) - ‘eng.train’ saved [3281528/3281528]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hOjXY3151HY1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422668576,"user_tz":-180,"elapsed":830,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["!mkdir datasets && mv eng.* datasets/"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcS3ocoT1HZF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1600422672651,"user_tz":-180,"elapsed":1230,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"523b3403-3e35-4db6-f1d0-868d6c38f13a"},"source":["!wget https://worksheets.codalab.org/rest/bundles/0x15a09c8f74f94a20bec0b68a2e6703b3/contents/blob/ && mkdir embeddings && mv index.html embeddings/glove.6B.100d.txt"],"execution_count":30,"outputs":[{"output_type":"stream","text":["--2020-09-18 09:51:14--  https://worksheets.codalab.org/rest/bundles/0x15a09c8f74f94a20bec0b68a2e6703b3/contents/blob/\n","Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.114.41.203\n","Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.114.41.203|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2020-09-18 09:51:14 ERROR 404: Not Found.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l_OvRsio1HZT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1600422685789,"user_tz":-180,"elapsed":2799,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"410304af-c1ef-40ae-9c1c-10155db8f206"},"source":["!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Cloning into 'a-PyTorch-Tutorial-to-Sequence-Labeling'...\n","remote: Enumerating objects: 6, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 139 (delta 1), reused 0 (delta 0), pack-reused 133\u001b[K\n","Receiving objects: 100% (139/139), 6.50 MiB | 30.80 MiB/s, done.\n","Resolving deltas: 100% (66/66), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wCBLwuyN1HZh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422713032,"user_tz":-180,"elapsed":805,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["!mkdir conll2003 && cp datasets/eng.testa conll2003/eng.testa.txt && cp datasets/eng.testb conll2003/eng.testb.txt && cp datasets/eng.train conll2003/eng.train.txt"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GnUvEDw1HZx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600422716910,"user_tz":-180,"elapsed":805,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"555a1e09-4533-46aa-8d4b-e2ec0340e700"},"source":["!mkdir .data && mv conll2003 .data/"],"execution_count":37,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘.data’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PjCLgtUc1HZ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600422701680,"user_tz":-180,"elapsed":780,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"ce606327-588a-4497-b605-790c2fca8acd"},"source":["!git clone https://github.com/kolloldas/torchnlp"],"execution_count":35,"outputs":[{"output_type":"stream","text":["fatal: destination path 'torchnlp' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d5CE8RnW1HaK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600422729125,"user_tz":-180,"elapsed":8237,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"5ca16aec-37aa-47b8-8b26-0289b297e571"},"source":["!cd torchnlp && pip3 install -r requirements.txt && python3 setup.py install"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Collecting git+git://github.com/kolloldas/text.git (from -r requirements.txt (line 4))\n","  Cloning git://github.com/kolloldas/text.git to /tmp/pip-req-build-qz1y640x\n","  Running command git clone -q git://github.com/kolloldas/text.git /tmp/pip-req-build-qz1y640x\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (4.41.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.6.4)\n","Collecting pytest-mock\n","  Downloading https://files.pythonhosted.org/packages/02/67/3d57cb4f92afea7bf77e5b19089dfc771bd05d3cbed13e32fbf7f097b562/pytest_mock-3.3.1-py3-none-any.whl\n","Collecting pytest-pythonpath\n","  Downloading https://files.pythonhosted.org/packages/46/c1/4b784495bb316962962df191b3c1b2302c60236301406283a9de1470786f/pytest-pythonpath-0.7.3.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.3.0->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (1.9.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (1.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (8.5.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (1.15.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (20.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (50.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->-r requirements.txt (line 9)) (0.7.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.3.0->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.3.0->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.3.0->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.3.0->-r requirements.txt (line 4)) (2020.6.20)\n","Building wheels for collected packages: pytest-pythonpath, torchtext\n","  Building wheel for pytest-pythonpath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytest-pythonpath: filename=pytest_pythonpath-0.7.3-cp36-none-any.whl size=3048 sha256=aa3996106a5a2da4d98fb4d2a87756b020b89b93d622d844a4d1fe748f844f8d\n","  Stored in directory: /root/.cache/pip/wheels/62/b3/f2/c71e3d66868019a971d07924a12a2d28e65694bfeaac95ac77\n","  Building wheel for torchtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchtext: filename=torchtext-0.3.0-cp36-none-any.whl size=57517 sha256=970cc2a6a7254c17f2670622f4f1dc39fcab3265f941b565ca5f97733410198d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6138ihyr/wheels/e9/73/43/84b2351c8dc2cfb34334d033b497dd0161e9c3d4b6d3a0443d\n","Successfully built pytest-pythonpath torchtext\n","\u001b[31mERROR: pytest-mock 3.3.1 has requirement pytest>=5.0, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","Installing collected packages: pytest-mock, pytest-pythonpath, torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed pytest-mock-3.3.1 pytest-pythonpath-0.7.3 torchtext-0.3.0\n","running install\n","running bdist_egg\n","running egg_info\n","creating torchnlp.egg-info\n","writing torchnlp.egg-info/PKG-INFO\n","writing dependency_links to torchnlp.egg-info/dependency_links.txt\n","writing top-level names to torchnlp.egg-info/top_level.txt\n","writing manifest file 'torchnlp.egg-info/SOURCES.txt'\n","writing manifest file 'torchnlp.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build\n","creating build/lib\n","creating build/lib/torchnlp\n","copying torchnlp/ner.py -> build/lib/torchnlp\n","copying torchnlp/chunk.py -> build/lib/torchnlp\n","copying torchnlp/__init__.py -> build/lib/torchnlp\n","creating build/lib/torchnlp/modules\n","copying torchnlp/modules/__init__.py -> build/lib/torchnlp/modules\n","copying torchnlp/modules/outputs.py -> build/lib/torchnlp/modules\n","copying torchnlp/modules/crf.py -> build/lib/torchnlp/modules\n","copying torchnlp/modules/normalization.py -> build/lib/torchnlp/modules\n","creating build/lib/torchnlp/common\n","copying torchnlp/common/hparams.py -> build/lib/torchnlp/common\n","copying torchnlp/common/model.py -> build/lib/torchnlp/common\n","copying torchnlp/common/__init__.py -> build/lib/torchnlp/common\n","copying torchnlp/common/cmd.py -> build/lib/torchnlp/common\n","copying torchnlp/common/prefs.py -> build/lib/torchnlp/common\n","copying torchnlp/common/info.py -> build/lib/torchnlp/common\n","copying torchnlp/common/train.py -> build/lib/torchnlp/common\n","copying torchnlp/common/evaluation.py -> build/lib/torchnlp/common\n","creating build/lib/torchnlp/data\n","copying torchnlp/data/inputs.py -> build/lib/torchnlp/data\n","copying torchnlp/data/nyt.py -> build/lib/torchnlp/data\n","copying torchnlp/data/__init__.py -> build/lib/torchnlp/data\n","copying torchnlp/data/conll.py -> build/lib/torchnlp/data\n","creating build/lib/torchnlp/tasks\n","copying torchnlp/tasks/__init__.py -> build/lib/torchnlp/tasks\n","creating build/lib/torchnlp/modules/transformer\n","copying torchnlp/modules/transformer/sublayers.py -> build/lib/torchnlp/modules/transformer\n","copying torchnlp/modules/transformer/main.py -> build/lib/torchnlp/modules/transformer\n","copying torchnlp/modules/transformer/layers.py -> build/lib/torchnlp/modules/transformer\n","copying torchnlp/modules/transformer/__init__.py -> build/lib/torchnlp/modules/transformer\n","creating build/lib/torchnlp/tasks/sequence_tagging\n","copying torchnlp/tasks/sequence_tagging/main.py -> build/lib/torchnlp/tasks/sequence_tagging\n","copying torchnlp/tasks/sequence_tagging/bilstm_tagger.py -> build/lib/torchnlp/tasks/sequence_tagging\n","copying torchnlp/tasks/sequence_tagging/transformer_tagger.py -> build/lib/torchnlp/tasks/sequence_tagging\n","copying torchnlp/tasks/sequence_tagging/__init__.py -> build/lib/torchnlp/tasks/sequence_tagging\n","copying torchnlp/tasks/sequence_tagging/tagger.py -> build/lib/torchnlp/tasks/sequence_tagging\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/torchnlp\n","copying build/lib/torchnlp/ner.py -> build/bdist.linux-x86_64/egg/torchnlp\n","creating build/bdist.linux-x86_64/egg/torchnlp/modules\n","copying build/lib/torchnlp/modules/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/modules\n","creating build/bdist.linux-x86_64/egg/torchnlp/modules/transformer\n","copying build/lib/torchnlp/modules/transformer/sublayers.py -> build/bdist.linux-x86_64/egg/torchnlp/modules/transformer\n","copying build/lib/torchnlp/modules/transformer/main.py -> build/bdist.linux-x86_64/egg/torchnlp/modules/transformer\n","copying build/lib/torchnlp/modules/transformer/layers.py -> build/bdist.linux-x86_64/egg/torchnlp/modules/transformer\n","copying build/lib/torchnlp/modules/transformer/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/modules/transformer\n","copying build/lib/torchnlp/modules/outputs.py -> build/bdist.linux-x86_64/egg/torchnlp/modules\n","copying build/lib/torchnlp/modules/crf.py -> build/bdist.linux-x86_64/egg/torchnlp/modules\n","copying build/lib/torchnlp/modules/normalization.py -> build/bdist.linux-x86_64/egg/torchnlp/modules\n","creating build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/hparams.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/model.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/cmd.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/prefs.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/info.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/train.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/common/evaluation.py -> build/bdist.linux-x86_64/egg/torchnlp/common\n","copying build/lib/torchnlp/chunk.py -> build/bdist.linux-x86_64/egg/torchnlp\n","creating build/bdist.linux-x86_64/egg/torchnlp/data\n","copying build/lib/torchnlp/data/inputs.py -> build/bdist.linux-x86_64/egg/torchnlp/data\n","copying build/lib/torchnlp/data/nyt.py -> build/bdist.linux-x86_64/egg/torchnlp/data\n","copying build/lib/torchnlp/data/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/data\n","copying build/lib/torchnlp/data/conll.py -> build/bdist.linux-x86_64/egg/torchnlp/data\n","copying build/lib/torchnlp/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp\n","creating build/bdist.linux-x86_64/egg/torchnlp/tasks\n","creating build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/sequence_tagging/main.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/sequence_tagging/bilstm_tagger.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/sequence_tagging/transformer_tagger.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/sequence_tagging/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/sequence_tagging/tagger.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging\n","copying build/lib/torchnlp/tasks/__init__.py -> build/bdist.linux-x86_64/egg/torchnlp/tasks\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/ner.py to ner.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/transformer/sublayers.py to sublayers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/transformer/main.py to main.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/transformer/layers.py to layers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/transformer/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/outputs.py to outputs.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/crf.py to crf.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/modules/normalization.py to normalization.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/hparams.py to hparams.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/model.py to model.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/cmd.py to cmd.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/prefs.py to prefs.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/info.py to info.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/train.py to train.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/common/evaluation.py to evaluation.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/chunk.py to chunk.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/data/inputs.py to inputs.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/data/nyt.py to nyt.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/data/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/data/conll.py to conll.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging/main.py to main.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging/bilstm_tagger.py to bilstm_tagger.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging/transformer_tagger.py to transformer_tagger.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/sequence_tagging/tagger.py to tagger.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/torchnlp/tasks/__init__.py to __init__.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying torchnlp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying torchnlp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying torchnlp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying torchnlp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating dist\n","creating 'dist/torchnlp-0.1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing torchnlp-0.1.0-py3.6.egg\n","Copying torchnlp-0.1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n","Adding torchnlp 0.1.0 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.6/dist-packages/torchnlp-0.1.0-py3.6.egg\n","Processing dependencies for torchnlp==0.1.0\n","Finished processing dependencies for torchnlp==0.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8gF119Ux1HaY","colab_type":"text"},"source":["#### Spacy "]},{"cell_type":"code","metadata":{"id":"NdW6ADlQ1Hab","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422741587,"user_tz":-180,"elapsed":1976,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import spacy\n","from spacy import displacy\n","from collections import Counter\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMd8o_m91Haq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600422743018,"user_tz":-180,"elapsed":523,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"46a5c721-d930-4460-8130-ac36312830af"},"source":["doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n","print([(X.text, X.label_) for X in doc.ents])"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5w5bFX_Q1Ha5","colab_type":"text"},"source":["Выкачаем статью и найдём в ней именованные сущности, выведем их число:"]},{"cell_type":"code","metadata":{"id":"_qmUrWD11Ha8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422751602,"user_tz":-180,"elapsed":938,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["from bs4 import BeautifulSoup\n","import requests\n","import re\n","def url_to_string(url):\n","    res = requests.get(url)\n","    html = res.text\n","    soup = BeautifulSoup(html, 'html.parser')\n","    for script in soup([\"script\", \"style\", 'aside']):\n","        script.extract()\n","    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUNyG8Hr1HbL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422756109,"user_tz":-180,"elapsed":821,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n","article = nlp(ny_bb)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGgwNFyS1Hba","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600422759926,"user_tz":-180,"elapsed":2289,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"c5271d86-dc6d-450f-8d1d-77f959ea7c5c"},"source":["len(article.ents)"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"9f4Z7r441Hb4","colab_type":"text"},"source":["Выведем число встреченных сущностей каждого типа:"]},{"cell_type":"code","metadata":{"id":"Rq7w_s_b1Hb7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600422768237,"user_tz":-180,"elapsed":811,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"14051b78-9e3f-4a5b-9ba3-037bd158a484"},"source":["labels = [x.label_ for x in article.ents]\n","Counter(labels)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'ORG': 1})"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"s3_EFC3V1HcK","colab_type":"text"},"source":["Выведем текст с подсвеченными сущностями разных типов:"]},{"cell_type":"code","metadata":{"id":"QzwaNm9z1HcM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600422772627,"user_tz":-180,"elapsed":728,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"3c1b67a5-4720-41f9-efe8-457b75dab01a"},"source":["sentences = [x for x in article.sents]\n","displacy.render(nlp(str(sentences)), jupyter=True, style='ent')"],"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">[nytimes.comPlease enable \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    JS\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," and disable any ad blocker]</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"fC8XCdrZ1Hcg","colab_type":"text"},"source":["### Natasha"]},{"cell_type":"code","metadata":{"id":"kTqIzWSB1Hcj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422806760,"user_tz":-180,"elapsed":552,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["import natasha"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-GP2fLk1Hcy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600422846647,"user_tz":-180,"elapsed":5575,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}}},"source":["l = natasha.MoneyExtractor"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7uNa5dC1HdE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":325},"executionInfo":{"status":"error","timestamp":1600422857377,"user_tz":-180,"elapsed":989,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"b353c1cd-a0f4-40aa-8bbd-06dbc88931d5"},"source":["l('я живу в Москве, но иногда езжу в Питер')"],"execution_count":52,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-de63dfe2af4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'я живу в Москве, но иногда езжу в Питер'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/natasha/extractors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, morph)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgrammars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoney\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMONEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mExtractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMONEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/natasha/extractors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rule, morph)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mExtractor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/natasha/extractors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rule, morph)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMorphTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mYargyParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rule, tokenizer, tagger)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/rule/constructors.py\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransformators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivateTransformator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mActivateTransformator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/rule/transformators.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/visitor.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/rule/transformators.py\u001b[0m in \u001b[0;36mvisit_Production\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisit_Production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/rule/transformators.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisit_Production\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/rule/transformators.py\u001b[0m in \u001b[0;36mvisit_term\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisit_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_predicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/predicates/constructors.py\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     72\u001b[0m         return self.__class__(\n\u001b[1;32m     73\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/predicates/constructors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, predicates)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mpredicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpredicate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0massert_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPredicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/predicates/constructors.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         return self.__class__(\n\u001b[1;32m     73\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/predicates/bank.py\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDictionaryPredicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/morph.py\u001b[0m in \u001b[0;36mnormalized\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/yargy/morph.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprepare_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'parse'"]}]},{"cell_type":"markdown","metadata":{"id":"1E1tAyYU1HeP","colab_type":"text"},"source":["#### LSTM-CRF"]},{"cell_type":"code","metadata":{"id":"whMQQwUs1HeR","colab_type":"code","colab":{}},"source":["import time\n","import torch\n","import torch.optim as optim\n","import os\n","import sys\n","\n","sys.path.append('./a-PyTorch-Tutorial-to-Sequence-Labeling')\n","\n","from models import LM_LSTM_CRF, ViterbiLoss\n","from utils import *\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from datasets import WCDataset\n","from inference import ViterbiDecoder\n","from sklearn.metrics import f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnNYtWq61Hec","colab_type":"code","colab":{}},"source":["#!touch ./a-PyTorch-Tutorial-to-Sequence-Labeling/__init__.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05mCaaN51Hel","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"ok","timestamp":1589305314771,"user_tz":-180,"elapsed":394727,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"2c769023-4efa-4205-f0f1-a05f5d4c5f53"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-05-12 17:35:21--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2020-05-12 17:35:22--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2020-05-12 17:35:22--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  2.07MB/s    in 6m 29s  \n","\n","2020-05-12 17:41:52 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"61LAfAHY_QC1","colab_type":"code","colab":{}},"source":["import zipfile\n","with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n","    zip_ref.extractall('.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jfczMS321Hex","colab_type":"code","colab":{}},"source":["# Data parameters\n","task = 'ner'  # tagging task, to choose column in CoNLL 2003 dataset\n","train_file = './datasets/eng.train'  # path to training data\n","val_file = './datasets/eng.testa'  # path to validation data\n","test_file = './datasets/eng.testb'  # path to test data\n","emb_file = './glove.6B.100d.txt'  # path to pre-trained word embeddings\n","min_word_freq = 5  # threshold for word frequency\n","min_char_freq = 1  # threshold for character frequency\n","caseless = True  # lowercase everything?\n","expand_vocab = True  # expand model's input vocabulary to the pre-trained embeddings' vocabulary?\n","\n","# Model parameters\n","char_emb_dim = 30  # character embedding size\n","with open(emb_file, 'r') as f:\n","    word_emb_dim = len(f.readline().split(' ')) - 1  # word embedding size\n","word_rnn_dim = 300  # word RNN size\n","char_rnn_dim = 300  # character RNN size\n","char_rnn_layers = 1  # number of layers in character RNN\n","word_rnn_layers = 1  # number of layers in word RNN\n","highway_layers = 1  # number of layers in highway network\n","dropout = 0.5  # dropout\n","fine_tune_word_embeddings = False  # fine-tune pre-trained word embeddings?\n","\n","# Training parameters\n","start_epoch = 0  # start at this epoch\n","batch_size = 10  # batch size\n","lr = 0.015  # learning rate\n","lr_decay = 0.05  # decay learning rate by this amount\n","momentum = 0.9  # momentum\n","workers = 1  # number of workers for loading data in the DataLoader\n","epochs = 10  # number of epochs to run without early-stopping\n","grad_clip = 5.  # clip gradients at this value\n","print_freq = 100  # print training or validation status every __ batches\n","best_f1 = 0.  # F1 score to start with\n","checkpoint = None  # path to model checkpoint, None if none\n","\n","tag_ind = 1 if task == 'pos' else 3  # choose column in CoNLL 2003 dataset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUQr-K-S1HfA","colab_type":"code","colab":{}},"source":["def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n","    \"\"\"\n","    Performs one epoch's training.\n","    :param train_loader: DataLoader for training data\n","    :param model: model\n","    :param lm_criterion: cross entropy loss layer\n","    :param crf_criterion: viterbi loss layer\n","    :param optimizer: optimizer\n","    :param epoch: epoch number\n","    :param vb_decoder: viterbi decoder (to decode and find F1 score)\n","    \"\"\"\n","\n","    model.train()  # training mode enables dropout\n","\n","    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n","    data_time = AverageMeter()  # data loading time per batch\n","    ce_losses = AverageMeter()  # cross entropy loss\n","    vb_losses = AverageMeter()  # viterbi loss\n","    f1s = AverageMeter()  # f1 score\n","\n","    start = time.time()\n","\n","    # Batches\n","    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n","            train_loader):\n","\n","        data_time.update(time.time() - start)\n","\n","        max_word_len = max(wmap_lengths.tolist())\n","        max_char_len = max(cmap_lengths.tolist())\n","\n","        # Reduce batch's padded length to maximum in-batch sequence\n","        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n","        wmaps = wmaps[:, :max_word_len].to(device)\n","        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n","        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n","        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n","        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n","        tmaps = tmaps[:, :max_word_len].to(device)\n","        wmap_lengths = wmap_lengths.to(device)\n","        cmap_lengths = cmap_lengths.to(device)\n","\n","        # Forward prop.\n","        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n","                                                                                                             cmaps_b,\n","                                                                                                             cmarkers_f,\n","                                                                                                             cmarkers_b,\n","                                                                                                             wmaps,\n","                                                                                                             tmaps,\n","                                                                                                             wmap_lengths,\n","                                                                                                             cmap_lengths)\n","\n","        # LM loss\n","\n","        # We don't predict the next word at the pads or <end> tokens\n","        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n","        # So, prediction lengths are word sequence lengths - 1\n","        lm_lengths = wmap_lengths_sorted - 1\n","        lm_lengths = lm_lengths.tolist()\n","\n","        # Remove scores at timesteps we won't predict at\n","        # pack_padded_sequence is a good trick to do this (see dynamic_rnn.py, where we explore this)\n","        lm_f_scores, _ = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n","        lm_b_scores, _ = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n","\n","        # For the forward sequence, targets are from the second word onwards, up to <end>\n","        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n","        lm_f_targets = wmaps_sorted[:, 1:]\n","        lm_f_targets, _ = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n","\n","        # For the backward sequence, targets are <end> followed by all words except the last word\n","        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n","        lm_b_targets = torch.cat(\n","            [torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), wmaps_sorted], dim=1)\n","        lm_b_targets, _ = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n","\n","        # Calculate loss\n","        ce_loss = lm_criterion(lm_f_scores, lm_f_targets) + lm_criterion(lm_b_scores, lm_b_targets)\n","        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n","        loss = ce_loss + vb_loss\n","\n","        # Back prop.\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        if grad_clip is not None:\n","            clip_gradient(optimizer, grad_clip)\n","\n","        optimizer.step()\n","\n","        # Viterbi decode to find accuracy / f1\n","        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n","\n","        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n","        decoded, _ = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n","        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n","        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n","\n","        # F1\n","        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n","\n","        # Keep track of metrics\n","        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n","        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n","        batch_time.update(time.time() - start)\n","        f1s.update(f1, sum(lm_lengths))\n","\n","        start = time.time()\n","\n","        # Print training status\n","        if i % print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n","                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n","                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n","                                                          batch_time=batch_time,\n","                                                          data_time=data_time, ce_loss=ce_losses,\n","                                                          vb_loss=vb_losses, f1=f1s))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DI8yNYYP1HfR","colab_type":"code","colab":{}},"source":["def validate(val_loader, model, crf_criterion, vb_decoder):\n","    \"\"\"\n","    Performs one epoch's validation.\n","    :param val_loader: DataLoader for validation data\n","    :param model: model\n","    :param crf_criterion: viterbi loss layer\n","    :param vb_decoder: viterbi decoder\n","    :return: validation F1 score\n","    \"\"\"\n","    model.eval()\n","\n","    batch_time = AverageMeter()\n","    vb_losses = AverageMeter()\n","    f1s = AverageMeter()\n","\n","    start = time.time()\n","\n","    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n","            val_loader):\n","\n","        max_word_len = max(wmap_lengths.tolist())\n","        max_char_len = max(cmap_lengths.tolist())\n","\n","        # Reduce batch's padded length to maximum in-batch sequence\n","        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n","        wmaps = wmaps[:, :max_word_len].to(device)\n","        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n","        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n","        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n","        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n","        tmaps = tmaps[:, :max_word_len].to(device)\n","        wmap_lengths = wmap_lengths.to(device)\n","        cmap_lengths = cmap_lengths.to(device)\n","\n","        # Forward prop.\n","        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n","                                                                                   cmaps_b,\n","                                                                                   cmarkers_f,\n","                                                                                   cmarkers_b,\n","                                                                                   wmaps,\n","                                                                                   tmaps,\n","                                                                                   wmap_lengths,\n","                                                                                   cmap_lengths)\n","\n","        # Viterbi / CRF layer loss\n","        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n","\n","        # Viterbi decode to find accuracy / f1\n","        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n","\n","        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n","        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n","        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n","        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n","\n","        # f1\n","        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n","\n","        # Keep track of metrics\n","        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n","        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n","        batch_time.update(time.time() - start)\n","\n","        start = time.time()\n","\n","        if i % print_freq == 0:\n","            print('Validation: [{0}/{1}]\\t'\n","                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n","                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n","                                                                  vb_loss=vb_losses, f1=f1s))\n","\n","    print(\n","        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n","                                                                          f1=f1s))\n","\n","    return f1s.avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tfz8w3o01Hfg","colab_type":"code","colab":{}},"source":["def main_func():\n","    \"\"\"\n","    Training and validation.\n","    \"\"\"\n","    global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n","\n","    # Read training and validation data\n","    train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n","    val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n","\n","    # Initialize model or load checkpoint\n","    if checkpoint is not None:\n","        checkpoint = torch.load(checkpoint)\n","        model = checkpoint['model']\n","        optimizer = checkpoint['optimizer']\n","        word_map = checkpoint['word_map']\n","        lm_vocab_size = checkpoint['lm_vocab_size']\n","        tag_map = checkpoint['tag_map']\n","        char_map = checkpoint['char_map']\n","        start_epoch = checkpoint['epoch'] + 1\n","        best_f1 = checkpoint['f1']\n","    else:\n","        word_map, char_map, tag_map = create_maps(train_words + val_words, train_tags + val_tags, min_word_freq,\n","                                                  min_char_freq)  # create word, char, tag maps\n","        embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map,\n","                                                              expand_vocab)  # load pre-trained embeddings\n","\n","        model = LM_LSTM_CRF(tagset_size=len(tag_map),\n","                            charset_size=len(char_map),\n","                            char_emb_dim=char_emb_dim,\n","                            char_rnn_dim=char_rnn_dim,\n","                            char_rnn_layers=char_rnn_layers,\n","                            vocab_size=len(word_map),\n","                            lm_vocab_size=lm_vocab_size,\n","                            word_emb_dim=word_emb_dim,\n","                            word_rnn_dim=word_rnn_dim,\n","                            word_rnn_layers=word_rnn_layers,\n","                            dropout=dropout,\n","                            highway_layers=highway_layers).to(device)\n","        model.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\n","        model.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\n","        optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n","\n","    # Loss functions\n","    lm_criterion = nn.CrossEntropyLoss().to(device)\n","    crf_criterion = ViterbiLoss(tag_map).to(device)\n","\n","    # Since the language model's vocab is restricted to in-corpus indices, encode training/val with only these!\n","    # word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n","    # they were in the pre-trained embeddings\n","    temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n","    train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n","                                        tag_map)\n","    val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n","\n","    # DataLoaders\n","    train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n","                                               num_workers=workers, pin_memory=False)\n","    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n","                                             num_workers=workers, pin_memory=False)\n","\n","    # Viterbi decoder (to find accuracy during validation)\n","    vb_decoder = ViterbiDecoder(tag_map)\n","\n","    # Epochs\n","    for epoch in range(start_epoch, epochs):\n","\n","        # One epoch's training\n","        train(train_loader=train_loader,\n","              model=model,\n","              lm_criterion=lm_criterion,\n","              crf_criterion=crf_criterion,\n","              optimizer=optimizer,\n","              epoch=epoch,\n","              vb_decoder=vb_decoder)\n","\n","        # One epoch's validation\n","        val_f1 = validate(val_loader=val_loader,\n","                          model=model,\n","                          crf_criterion=crf_criterion,\n","                          vb_decoder=vb_decoder)\n","\n","        # Did validation F1 score improve?\n","        is_best = val_f1 > best_f1\n","        best_f1 = max(val_f1, best_f1)\n","        if not is_best:\n","            epochs_since_improvement += 1\n","            print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n","        else:\n","            epochs_since_improvement = 0\n","\n","        # Save checkpoint\n","        save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n","\n","        # Decay learning rate every epoch\n","        adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzzHHMSo1Hfq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1589306347817,"user_tz":-180,"elapsed":30821,"user":{"displayName":"Dmitry Ilvovsky","photoUrl":"","userId":"04607260023147477501"}},"outputId":"b19407d9-e315-473d-b079-9932ce1bcc81"},"source":["main_func()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Embedding length is 100.\n","You have elected to include embeddings that are out-of-corpus.\n","\n","Loading embeddings...\n","'word_map' is being updated accordingly.\n","\n","Done.\n"," Embedding vocabulary: 400054\n"," Language Model vocabulary: 4671.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-6bf2be90a1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-47-eaa394b665c2>\u001b[0m in \u001b[0;36mmain_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m               \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m               vb_decoder=vb_decoder)\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# One epoch's validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-aac0522bd74d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_f_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_f_targets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlm_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_b_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_b_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mvb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmaps_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwmap_lengths_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvb_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 2125\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   2126\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected target size (10, 4671), got torch.Size([10, 37])"]}]},{"cell_type":"code","metadata":{"id":"JjmWlhoU1Hf2","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}